#blocked = #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 8], warpsPerCTA = [8, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 8], order = [0, 1]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [2, 32], warpsPerCTA = [8, 1], order = [1, 0]}>
#linear = #ttg.linear<{register = [[0, 1], [0, 2], [0, 4], [0, 16]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 8]], warp = [[0, 32], [32, 0], [64, 0]], block = []}>
#loc = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":50:0)
#loc1 = loc(unknown)
#loc18 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":116:68)
#loc20 = loc("left")
#loc21 = loc("right")
#loc114 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":310:35)
#loc123 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":319:21)
#mma = #ttg.amd_mfma<{version = 4, warpsPerCTA = [8, 1], instrShape = [32, 32], isTransposed = true}>
#mma1 = #ttg.amd_mfma<{version = 4, warpsPerCTA = [4, 2], instrShape = [32, 32], isTransposed = true}>
#shared = #ttg.swizzled_shared<{vec = 8, perPhase = 2, maxPhase = 8, order = [1, 0]}>
#shared1 = #ttg.swizzled_shared<{vec = 8, perPhase = 2, maxPhase = 8, order = [0, 1]}>
#shared2 = #ttg.swizzled_shared<{vec = 4, perPhase = 2, maxPhase = 8, order = [1, 0]}>
#shared3 = #ttg.swizzled_shared<{vec = 4, perPhase = 1, maxPhase = 16, order = [0, 1]}>
#smem = #ttg.shared_memory
#loc139 = loc("output_ptr"(#loc))
#loc140 = loc("query_ptr"(#loc))
#loc141 = loc("key_cache_ptr"(#loc))
#loc142 = loc("value_cache_ptr"(#loc))
#loc143 = loc("sink_ptr"(#loc))
#loc144 = loc("block_tables_ptr"(#loc))
#loc145 = loc("seq_lens_ptr"(#loc))
#loc146 = loc("scale"(#loc))
#loc147 = loc("k_scale"(#loc))
#loc148 = loc("v_scale"(#loc))
#loc149 = loc("out_scale"(#loc))
#loc150 = loc("softcap"(#loc))
#loc151 = loc("block_table_stride"(#loc))
#loc152 = loc("query_stride_0"(#loc))
#loc153 = loc("query_stride_1"(#loc))
#loc154 = loc("output_stride_0"(#loc))
#loc155 = loc("output_stride_1"(#loc))
#loc156 = loc("qq_bias_stride_0"(#loc))
#loc157 = loc("stride_k_cache_0"(#loc))
#loc158 = loc("stride_k_cache_1"(#loc))
#loc159 = loc("stride_k_cache_2"(#loc))
#loc160 = loc("stride_v_cache_0"(#loc))
#loc161 = loc("stride_v_cache_1"(#loc))
#loc162 = loc("stride_v_cache_2"(#loc))
#loc163 = loc("query_start_len_ptr"(#loc))
#loc164 = loc("num_seqs"(#loc))
#loc168 = loc("seq_idx"(#loc18))
#loc169 = loc("left"(#loc20))
#loc170 = loc("right"(#loc21))
#loc253 = loc("m_j"(#loc114))
#loc260 = loc("l_j"(#loc123))
#loc274 = loc(callsite(#loc169 at #loc168))
#loc275 = loc(callsite(#loc170 at #loc168))
#loc292 = loc(callsite(#loc1 at #loc253))
#loc294 = loc(callsite(#loc1 at #loc260))
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 8 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @kernel_unified_attention_2d(%output_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("output_ptr"(#loc)), %query_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("query_ptr"(#loc)), %key_cache_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("key_cache_ptr"(#loc)), %value_cache_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("value_cache_ptr"(#loc)), %sink_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("sink_ptr"(#loc)), %block_tables_ptr: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("block_tables_ptr"(#loc)), %seq_lens_ptr: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("seq_lens_ptr"(#loc)), %scale: f32 loc("scale"(#loc)), %k_scale: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("k_scale"(#loc)), %v_scale: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("v_scale"(#loc)), %out_scale: f32 loc("out_scale"(#loc)), %softcap: i32 {tt.divisibility = 16 : i32} loc("softcap"(#loc)), %block_table_stride: i64 {tt.divisibility = 16 : i32} loc("block_table_stride"(#loc)), %query_stride_0: i64 {tt.divisibility = 16 : i32} loc("query_stride_0"(#loc)), %query_stride_1: i64 {tt.divisibility = 16 : i32} loc("query_stride_1"(#loc)), %output_stride_0: i64 {tt.divisibility = 16 : i32} loc("output_stride_0"(#loc)), %output_stride_1: i64 {tt.divisibility = 16 : i32} loc("output_stride_1"(#loc)), %qq_bias_stride_0: i64 {tt.divisibility = 16 : i32} loc("qq_bias_stride_0"(#loc)), %stride_k_cache_0: i64 {tt.divisibility = 16 : i32} loc("stride_k_cache_0"(#loc)), %stride_k_cache_1: i64 {tt.divisibility = 16 : i32} loc("stride_k_cache_1"(#loc)), %stride_k_cache_2: i64 {tt.divisibility = 16 : i32} loc("stride_k_cache_2"(#loc)), %stride_v_cache_0: i64 {tt.divisibility = 16 : i32} loc("stride_v_cache_0"(#loc)), %stride_v_cache_1: i64 {tt.divisibility = 16 : i32} loc("stride_v_cache_1"(#loc)), %stride_v_cache_2: i64 {tt.divisibility = 16 : i32} loc("stride_v_cache_2"(#loc)), %query_start_len_ptr: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("query_start_len_ptr"(#loc)), %num_seqs: i32 loc("num_seqs"(#loc))) attributes {noinline = false} {
    %true = arith.constant true loc(#loc1)
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %cst = arith.constant dense<8> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_0 = arith.constant dense<64> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_1 = arith.constant dense<1> : tensor<128x1xi32, #mma> loc(#loc1)
    %cst_2 = arith.constant dense<0xFF80> : tensor<128xbf16, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_3 = arith.constant dense<64> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
    %cst_4 = arith.constant dense<0> : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc1)
    %cst_5 = arith.constant dense<0> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc1)
    %cst_6 = arith.constant dense<1> : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc1)
    %cst_7 = arith.constant dense<1> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc1)
    %cst_8 = arith.constant dense<64> : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc1)
    %cst_9 = arith.constant dense<64> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc1)
    %c3_i32 = arith.constant 3 : i32 loc(#loc1)
    %cst_10 = arith.constant dense<1.000000e+00> : tensor<128x1xf32, #mma> loc(#loc1)
    %cst_11 = arith.constant dense<0.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_12 = arith.constant dense<0xFF800000> : tensor<128x64xf32, #mma> loc(#loc1)
    %cst_13 = arith.constant dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_14 = arith.constant dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_15 = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #mma1> loc(#loc1)
    %cst_16 = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #mma> loc(#loc1)
    %c2_i32 = arith.constant 2 : i32 loc(#loc1)
    %c63_i32 = arith.constant 63 : i32 loc(#loc1)
    %c64_i32 = arith.constant 64 : i32 loc(#loc1)
    %cst_17 = arith.constant dense<0.000000e+00> : tensor<128x64xbf16, #blocked> loc(#loc1)
    %c16_i32 = arith.constant 16 : i32 loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %c8_i32 = arith.constant 8 : i32 loc(#loc1)
    %cst_18 = arith.constant dense<8> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
    %cst_19 = arith.constant dense<8> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc1)
    %cst_20 = arith.constant dense<64> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc1)
    %cst_21 = arith.constant dense<64> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc1)
    %cst_22 = arith.constant dense<1> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc1)
    %cst_23 = arith.constant dense<1> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc1)
    %cst_24 = arith.constant dense<0> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc1)
    %cst_25 = arith.constant dense<0> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc1)
    %cst_26 = arith.constant dense<64> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc1)
    %kv_head_idx = tt.get_program_id x : i32 loc(#loc165)
    %q_block_global_idx = tt.get_program_id y : i32 loc(#loc166)
    llvm.intr.assume %true : i1 loc(#loc4)
    llvm.intr.assume %true : i1 loc(#loc5)
    llvm.intr.assume %true : i1 loc(#loc6)
    llvm.intr.assume %true : i1 loc(#loc7)
    llvm.intr.assume %true : i1 loc(#loc8)
    llvm.intr.assume %true : i1 loc(#loc9)
    llvm.intr.assume %true : i1 loc(#loc10)
    llvm.intr.assume %true : i1 loc(#loc11)
    llvm.intr.assume %true : i1 loc(#loc12)
    llvm.intr.assume %true : i1 loc(#loc13)
    llvm.intr.assume %true : i1 loc(#loc14)
    llvm.intr.assume %true : i1 loc(#loc15)
    llvm.intr.assume %true : i1 loc(#loc16)
    %right:2 = scf.while (%left = %c0_i32, %right_337 = %num_seqs) : (i32, i32) -> (i32, i32) {
      %seq_idx_338 = arith.cmpi slt, %left, %right_337 : i32 loc(#loc273)
      scf.condition(%seq_idx_338) %left, %right_337 : i32, i32 loc(#loc273)
    } do {
    ^bb0(%left: i32 loc(callsite(#loc169 at #loc168)), %right_337: i32 loc(callsite(#loc170 at #loc168))):
      %mid = arith.addi %left, %right_337 : i32 loc(#loc276)
      %right_338 = arith.divsi %mid, %c2_i32 : i32 loc(#loc296)
      %val = tt.addptr %query_start_len_ptr, %right_338 : !tt.ptr<i32>, i32 loc(#loc278)
      %val_339 = tt.load %val : !tt.ptr<i32> loc(#loc279)
      %mid_val = arith.divsi %val_339, %c16_i32 : i32 loc(#loc280)
      %mid_val_340 = arith.addi %mid_val, %right_338 : i32 loc(#loc281)
      %seq_idx_341 = arith.cmpi sle, %mid_val_340, %q_block_global_idx : i32 loc(#loc282)
      %seq_idx_342 = arith.select %seq_idx_341, %right_337, %right_338 : i32 loc(#loc283)
      %seq_idx_343 = scf.if %seq_idx_341 -> (i32) {
        %left_344 = arith.addi %right_338, %c1_i32 : i32 loc(#loc297)
        scf.yield %left_344 : i32 loc(#loc297)
      } else {
        scf.yield %left : i32 loc(#loc285)
      } loc(#loc283)
      scf.yield %seq_idx_343, %seq_idx_342 : i32, i32 loc(#loc286)
    } loc(#loc295)
    %seq_idx = arith.subi %right#0, %c1_i32 : i32 loc(#loc287)
    %q_block_start_idx = tt.addptr %query_start_len_ptr, %seq_idx : !tt.ptr<i32>, i32 loc(#loc178)
    %q_block_start_idx_27 = tt.load %q_block_start_idx : !tt.ptr<i32> loc(#loc179)
    %q_block_start_idx_28 = arith.divsi %q_block_start_idx_27, %c16_i32 : i32 loc(#loc180)
    %q_block_start_idx_29 = arith.addi %q_block_start_idx_28, %seq_idx : i32 loc(#loc181)
    %q_block_local_idx = arith.subi %q_block_global_idx, %q_block_start_idx_29 : i32 loc(#loc182)
    %cur_batch_in_all_stop_index = tt.addptr %q_block_start_idx, %c1_i32 : !tt.ptr<i32>, i32 loc(#loc183)
    %cur_batch_in_all_stop_index_30 = tt.load %cur_batch_in_all_stop_index : !tt.ptr<i32> loc(#loc184)
    %cur_batch_query_len = arith.subi %cur_batch_in_all_stop_index_30, %q_block_start_idx_27 : i32 loc(#loc185)
    %0 = arith.muli %q_block_local_idx, %c16_i32 : i32 loc(#loc41)
    %1 = arith.cmpi sge, %0, %cur_batch_query_len : i32 loc(#loc42)
    cf.cond_br %1, ^bb1, ^bb2 loc(#loc42)
  ^bb1:  // pred: ^bb0
    tt.return loc(#loc43)
  ^bb2:  // pred: ^bb0
    %offs_m = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc186)
    %offs_m_31 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc186)
    %offs_m_32 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc186)
    %offs_d = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc187)
    %offs_d_33 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc187)
    %offs_d_34 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc187)
    %offs_d_35 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc187)
    %offs_d_36 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc187)
    %query_pos = arith.divsi %offs_m, %cst_19 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc188)
    %query_pos_37 = arith.divsi %offs_m_31, %cst_18 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc188)
    %query_pos_38 = arith.divsi %offs_m_32, %cst : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc188)
    %query_pos_39 = tt.splat %0 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc189)
    %query_pos_40 = tt.splat %0 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc189)
    %query_pos_41 = tt.splat %0 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc189)
    %query_pos_42 = arith.addi %query_pos_39, %query_pos : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc189)
    %query_pos_43 = arith.addi %query_pos_40, %query_pos_37 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc189)
    %query_pos_44 = arith.addi %query_pos_41, %query_pos_38 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc189)
    %query_offset_0 = tt.splat %q_block_start_idx_27 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc190)
    %query_offset_0_45 = tt.splat %q_block_start_idx_27 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc190)
    %query_offset_0_46 = arith.addi %query_offset_0, %query_pos_42 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc190)
    %query_offset_0_47 = arith.addi %query_offset_0_45, %query_pos_43 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc190)
    %query_offset_1 = arith.muli %kv_head_idx, %c8_i32 : i32 loc(#loc191)
    %query_offset_1_48 = arith.remsi %offs_m, %cst_19 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc192)
    %query_offset_1_49 = arith.remsi %offs_m_31, %cst_18 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc192)
    %query_offset_1_50 = arith.remsi %offs_m_32, %cst : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc192)
    %query_offset_1_51 = tt.splat %query_offset_1 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc193)
    %query_offset_1_52 = tt.splat %query_offset_1 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc193)
    %query_offset_1_53 = tt.splat %query_offset_1 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc193)
    %query_offset_1_54 = arith.addi %query_offset_1_51, %query_offset_1_48 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc193)
    %query_offset_1_55 = arith.addi %query_offset_1_52, %query_offset_1_49 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc193)
    %query_offset_1_56 = arith.addi %query_offset_1_53, %query_offset_1_50 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc193)
    %query_offset = tt.expand_dims %query_offset_0_46 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<128x1xi32, #linear> loc(#loc194)
    %query_offset_57 = tt.expand_dims %query_offset_0_47 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xi32, #blocked> loc(#loc194)
    %query_offset_58 = arith.extsi %query_offset : tensor<128x1xi32, #linear> to tensor<128x1xi64, #linear> loc(#loc195)
    %query_offset_59 = arith.extsi %query_offset_57 : tensor<128x1xi32, #blocked> to tensor<128x1xi64, #blocked> loc(#loc195)
    %query_offset_60 = tt.expand_dims %query_offset_1_54 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<128x1xi32, #linear> loc(#loc196)
    %query_offset_61 = tt.expand_dims %query_offset_1_55 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xi32, #blocked> loc(#loc196)
    %query_offset_62 = arith.extsi %query_offset_60 : tensor<128x1xi32, #linear> to tensor<128x1xi64, #linear> loc(#loc197)
    %query_offset_63 = arith.extsi %query_offset_61 : tensor<128x1xi32, #blocked> to tensor<128x1xi64, #blocked> loc(#loc197)
    %query_offset_64 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc198)
    %query_offset_65 = tt.expand_dims %offs_d {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> -> tensor<1x64xi32, #blocked2> loc(#loc198)
    %query_offset_66 = tt.expand_dims %offs_d_33 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> -> tensor<1x64xi32, #linear> loc(#loc198)
    %query_offset_67 = tt.expand_dims %offs_d_34 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi32, #blocked> loc(#loc198)
    %query_offset_68 = tt.expand_dims %query_offset_64 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x64xi32, #blocked1> loc(#loc198)
    %query_offset_69 = arith.extsi %query_offset_65 : tensor<1x64xi32, #blocked2> to tensor<1x64xi64, #blocked2> loc(#loc199)
    %query_offset_70 = arith.extsi %query_offset_66 : tensor<1x64xi32, #linear> to tensor<1x64xi64, #linear> loc(#loc199)
    %query_offset_71 = arith.extsi %query_offset_67 : tensor<1x64xi32, #blocked> to tensor<1x64xi64, #blocked> loc(#loc199)
    %query_offset_72 = arith.extsi %query_offset_68 : tensor<1x64xi32, #blocked1> to tensor<1x64xi64, #blocked1> loc(#loc199)
    %dim_mask = arith.cmpi slt, %offs_d, %cst_20 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc200)
    %dim_mask_73 = arith.cmpi slt, %offs_d_33, %cst_21 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc200)
    %dim_mask_74 = arith.cmpi slt, %offs_d_34, %cst_9 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc200)
    %dim_mask_75 = arith.cmpi slt, %offs_d_36, %cst_8 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc200)
    %dim_mask_76 = arith.select %dim_mask, %cst_22, %cst_24 : tensor<64xi1, #ttg.slice<{dim = 0, parent = #blocked2}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc201)
    %dim_mask_77 = arith.select %dim_mask_73, %cst_23, %cst_25 : tensor<64xi1, #ttg.slice<{dim = 0, parent = #linear}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc201)
    %dim_mask_78 = arith.select %dim_mask_74, %cst_7, %cst_5 : tensor<64xi1, #ttg.slice<{dim = 0, parent = #blocked}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc201)
    %dim_mask_79 = arith.select %dim_mask_75, %cst_6, %cst_4 : tensor<64xi1, #ttg.slice<{dim = 1, parent = #blocked1}>>, tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc201)
    %dim_mask_80 = arith.cmpi ne, %dim_mask_76, %cst_24 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc202)
    %dim_mask_81 = arith.cmpi ne, %dim_mask_77, %cst_25 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc202)
    %dim_mask_82 = arith.cmpi ne, %dim_mask_78, %cst_5 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc202)
    %dim_mask_83 = arith.cmpi ne, %dim_mask_79, %cst_4 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc202)
    %query_mask_0 = tt.splat %cur_batch_query_len : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc203)
    %query_mask_0_84 = tt.splat %cur_batch_query_len : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc203)
    %query_mask_0_85 = tt.splat %cur_batch_query_len : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc203)
    %query_mask_0_86 = arith.cmpi slt, %query_pos_42, %query_mask_0 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc203)
    %query_mask_0_87 = arith.cmpi slt, %query_pos_43, %query_mask_0_84 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc203)
    %query_mask_0_88 = arith.cmpi slt, %query_pos_44, %query_mask_0_85 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc203)
    %query_mask_1 = arith.cmpi slt, %query_offset_1_54, %cst_26 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc204)
    %query_mask_1_89 = arith.cmpi slt, %query_offset_1_55, %cst_3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc204)
    %query_mask_1_90 = arith.cmpi slt, %query_offset_1_56, %cst_0 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc204)
    %Q = tt.expand_dims %dim_mask_80 {axis = 0 : i32} : tensor<64xi1, #ttg.slice<{dim = 0, parent = #blocked2}>> -> tensor<1x64xi1, #blocked2> loc(#loc205)
    %Q_91 = tt.expand_dims %dim_mask_81 {axis = 0 : i32} : tensor<64xi1, #ttg.slice<{dim = 0, parent = #linear}>> -> tensor<1x64xi1, #linear> loc(#loc205)
    %Q_92 = tt.expand_dims %dim_mask_82 {axis = 0 : i32} : tensor<64xi1, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi1, #blocked> loc(#loc205)
    %Q_93 = tt.expand_dims %query_mask_0_86 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<128x1xi1, #linear> loc(#loc206)
    %Q_94 = tt.expand_dims %query_mask_0_87 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xi1, #blocked> loc(#loc206)
    %Q_95 = tt.expand_dims %query_mask_0_88 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xi1, #mma> loc(#loc206)
    %Q_96 = tt.broadcast %Q_91 : tensor<1x64xi1, #linear> -> tensor<128x64xi1, #linear> loc(#loc207)
    %Q_97 = tt.broadcast %Q_92 : tensor<1x64xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc207)
    %Q_98 = tt.broadcast %Q_93 : tensor<128x1xi1, #linear> -> tensor<128x64xi1, #linear> loc(#loc207)
    %Q_99 = tt.broadcast %Q_94 : tensor<128x1xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc207)
    %Q_100 = arith.andi %Q_96, %Q_98 : tensor<128x64xi1, #linear> loc(#loc207)
    %Q_101 = arith.andi %Q_97, %Q_99 : tensor<128x64xi1, #blocked> loc(#loc207)
    %Q_102 = tt.expand_dims %query_mask_1 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<128x1xi1, #linear> loc(#loc208)
    %Q_103 = tt.expand_dims %query_mask_1_89 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xi1, #blocked> loc(#loc208)
    %Q_104 = tt.expand_dims %query_mask_1_90 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xi1, #mma> loc(#loc208)
    %Q_105 = tt.broadcast %Q_102 : tensor<128x1xi1, #linear> -> tensor<128x64xi1, #linear> loc(#loc209)
    %Q_106 = tt.broadcast %Q_103 : tensor<128x1xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc209)
    %Q_107 = arith.andi %Q_100, %Q_105 : tensor<128x64xi1, #linear> loc(#loc209)
    %Q_108 = arith.andi %Q_101, %Q_106 : tensor<128x64xi1, #blocked> loc(#loc209)
    %Q_109 = tt.splat %query_stride_0 : i64 -> tensor<128x1xi64, #blocked> loc(#loc210)
    %Q_110 = arith.muli %query_offset_59, %Q_109 : tensor<128x1xi64, #blocked> loc(#loc210)
    %Q_111 = tt.splat %query_stride_1 : i64 -> tensor<128x1xi64, #blocked> loc(#loc210)
    %Q_112 = arith.muli %query_offset_63, %Q_111 : tensor<128x1xi64, #blocked> loc(#loc210)
    %Q_113 = arith.addi %Q_110, %Q_112 : tensor<128x1xi64, #blocked> loc(#loc210)
    %Q_114 = tt.broadcast %Q_113 : tensor<128x1xi64, #blocked> -> tensor<128x64xi64, #blocked> loc(#loc210)
    %Q_115 = tt.broadcast %query_offset_71 : tensor<1x64xi64, #blocked> -> tensor<128x64xi64, #blocked> loc(#loc210)
    %Q_116 = arith.addi %Q_114, %Q_115 : tensor<128x64xi64, #blocked> loc(#loc210)
    %Q_117 = arith.trunci %Q_116 : tensor<128x64xi64, #blocked> to tensor<128x64xi32, #blocked> loc(#loc210)
    %Q_118 = tt.splat %query_ptr : !tt.ptr<bf16> -> tensor<128x64x!tt.ptr<bf16>, #blocked> loc(#loc211)
    %Q_119 = tt.addptr %Q_118, %Q_117 : tensor<128x64x!tt.ptr<bf16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc211)
    %Q_120 = tt.load %Q_119, %Q_108, %cst_17 cacheModifier = cg : tensor<128x64x!tt.ptr<bf16>, #blocked> loc(#loc211)
    %Q_121 = ttg.local_alloc %Q_120 : (tensor<128x64xbf16, #blocked>) -> !ttg.memdesc<128x64xbf16, #shared, #smem> loc(#loc211)
    %Q_122 = ttg.local_load %Q_121 : !ttg.memdesc<128x64xbf16, #shared, #smem> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc211)
    %block_table_offset = arith.extsi %seq_idx : i32 to i64 loc(#loc212)
    %block_table_offset_123 = arith.muli %block_table_offset, %block_table_stride : i64 loc(#loc212)
    %M = tt.addptr %sink_ptr, %query_offset_1 : !tt.ptr<bf16>, i32 loc(#loc213)
    %M_124 = amdgpu.buffer_load %M[%query_offset_1_50], %query_mask_1_90, %cst_2 : tensor<128xbf16, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc214)
    %M_125 = arith.extf %M_124 : tensor<128xbf16, #ttg.slice<{dim = 1, parent = #mma}>> to tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc215)
    %seq_len = tt.addptr %seq_lens_ptr, %seq_idx : !tt.ptr<i32>, i32 loc(#loc216)
    %seq_len_126 = tt.load %seq_len : !tt.ptr<i32> loc(#loc217)
    %context_len = arith.subi %seq_len_126, %cur_batch_query_len : i32 loc(#loc218)
    %max_seq_prefix_len = arith.addi %context_len, %0 : i32 loc(#loc219)
    %max_seq_prefix_len_127 = arith.addi %max_seq_prefix_len, %c16_i32 : i32 loc(#loc220)
    %max_seq_prefix_len_128 = arith.minsi %max_seq_prefix_len_127, %seq_len_126 : i32 loc(#loc221)
    %num_blocks = arith.addi %max_seq_prefix_len_128, %c63_i32 : i32 loc(#loc288)
    %num_blocks_129 = arith.divsi %num_blocks, %c64_i32 : i32 loc(#loc289)
    %physical_block_idx = tt.addptr %block_tables_ptr, %block_table_offset_123 : !tt.ptr<i32>, i64 loc(#loc223)
    %acc = arith.cmpi sgt, %num_blocks_129, %c0_i32 : i32 loc(#loc298)
    %physical_block_idx_130 = tt.load %physical_block_idx, %acc {amd.pipeliner_part = "prologue"} : !tt.ptr<i32> loc(#loc225)
    %v_offset = arith.extsi %kv_head_idx : i32 to i64 loc(#loc226)
    %v_offset_131 = arith.muli %v_offset, %stride_v_cache_2 : i64 loc(#loc226)
    %v_offset_132 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc227)
    %v_offset_133 = tt.expand_dims %offs_d_36 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<64x1xi32, #blocked1> loc(#loc227)
    %v_offset_134 = tt.expand_dims %v_offset_132 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<64x1xi32, #blocked2> loc(#loc227)
    %v_offset_135 = arith.extsi %v_offset_133 : tensor<64x1xi32, #blocked1> to tensor<64x1xi64, #blocked1> loc(#loc228)
    %v_offset_136 = arith.extsi %v_offset_134 : tensor<64x1xi32, #blocked2> to tensor<64x1xi64, #blocked2> loc(#loc228)
    %k_offset = arith.muli %v_offset, %stride_k_cache_2 : i64 loc(#loc229)
    %K_load = tt.expand_dims %dim_mask_83 {axis = 1 : i32} : tensor<64xi1, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<64x1xi1, #blocked1> loc(#loc230)
    %K_load_137 = tt.broadcast %K_load : tensor<64x1xi1, #blocked1> -> tensor<64x64xi1, #blocked1> loc(#loc231)
    %V_load = tt.broadcast %Q : tensor<1x64xi1, #blocked2> -> tensor<64x64xi1, #blocked2> loc(#loc232)
    %seq_mask = tt.expand_dims %query_pos_44 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xi32, #mma> loc(#loc233)
    %seq_mask_138 = tt.splat %context_len : i32 -> tensor<128x1xi32, #mma> loc(#loc234)
    %seq_mask_139 = arith.addi %seq_mask_138, %seq_mask : tensor<128x1xi32, #mma> loc(#loc234)
    %seq_mask_140 = arith.addi %seq_mask_139, %cst_1 : tensor<128x1xi32, #mma> loc(#loc235)
    %seq_mask_141 = tt.broadcast %seq_mask_140 : tensor<128x1xi32, #mma> -> tensor<128x64xi32, #mma> loc(#loc236)
    %S = tt.splat %scale : f32 -> tensor<128x64xf32, #mma> loc(#loc237)
    %S_142 = arith.andi %Q_104, %Q_95 : tensor<128x1xi1, #mma> loc(#loc238)
    %S_143 = tt.broadcast %S_142 : tensor<128x1xi1, #mma> -> tensor<128x64xi1, #mma> loc(#loc239)
    %K_load_144 = ttg.local_alloc : () -> !ttg.memdesc<3x64x64xbf16, #shared1, #smem, mutable> loc(#loc231)
    %V_load_145 = ttg.local_alloc : () -> !ttg.memdesc<3x64x64xbf16, #shared2, #smem, mutable> loc(#loc232)
    %acc_146 = arith.cmpi sgt, %num_blocks_129, %c1_i32 : i32 loc(#loc298)
    %physical_block_idx_147 = tt.addptr %physical_block_idx, %c1_i32 : !tt.ptr<i32>, i32 loc(#loc240)
    %physical_block_idx_148 = tt.load %physical_block_idx_147, %acc_146 {amd.pipeliner_part = "prologue"} : !tt.ptr<i32> loc(#loc225)
    %v_offset_149 = arith.extsi %physical_block_idx_130 : i32 to i64 loc(#loc241)
    %v_offset_150 = arith.muli %v_offset_149, %stride_v_cache_0 : i64 loc(#loc241)
    %v_offset_151 = arith.addi %v_offset_150, %v_offset_131 : i64 loc(#loc242)
    %k_offset_152 = arith.muli %v_offset_149, %stride_k_cache_0 : i64 loc(#loc243)
    %k_offset_153 = arith.addi %k_offset_152, %k_offset : i64 loc(#loc244)
    %K_load_154 = tt.broadcast %v_offset_135 : tensor<64x1xi64, #blocked1> -> tensor<64x64xi64, #blocked1> loc(#loc245)
    %K_load_155 = tt.splat %stride_k_cache_1 : i64 -> tensor<1x64xi64, #blocked1> loc(#loc245)
    %K_load_156 = arith.muli %query_offset_72, %K_load_155 : tensor<1x64xi64, #blocked1> loc(#loc245)
    %K_load_157 = tt.broadcast %K_load_156 : tensor<1x64xi64, #blocked1> -> tensor<64x64xi64, #blocked1> loc(#loc245)
    %K_load_158 = arith.addi %K_load_154, %K_load_157 : tensor<64x64xi64, #blocked1> loc(#loc245)
    %K_load_159 = tt.addptr %key_cache_ptr, %k_offset_153 : !tt.ptr<bf16>, i64 loc(#loc245)
    %K_load_160 = arith.trunci %K_load_158 : tensor<64x64xi64, #blocked1> to tensor<64x64xi32, #blocked1> loc(#loc245)
    %K_load_161 = ttg.memdesc_index %K_load_144[%c0_i32] : !ttg.memdesc<3x64x64xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable, 3x64x64> loc(#loc231)
    %acc_162 = tt.splat %acc : i1 -> tensor<64x64xi1, #blocked1> loc(#loc298)
    %acc_163 = arith.andi %acc_162, %K_load_137 : tensor<64x64xi1, #blocked1> loc(#loc298)
    %K_load_164 = amdgpu.buffer_load_to_local %K_load_159[%K_load_160] mask = %acc_163 into %K_load_161 : <bf16>[tensor<64x64xi32, #blocked1>]  -> <64x64xbf16, #shared1, #smem, mutable, 3x64x64> loc(#loc231)
    %K_load_165 = ttg.async_commit_group tokens %K_load_164 loc(#loc231)
    %V_load_166 = tt.broadcast %query_offset_69 : tensor<1x64xi64, #blocked2> -> tensor<64x64xi64, #blocked2> loc(#loc246)
    %V_load_167 = tt.splat %stride_v_cache_1 : i64 -> tensor<64x1xi64, #blocked2> loc(#loc246)
    %V_load_168 = arith.muli %v_offset_136, %V_load_167 : tensor<64x1xi64, #blocked2> loc(#loc246)
    %V_load_169 = tt.broadcast %V_load_168 : tensor<64x1xi64, #blocked2> -> tensor<64x64xi64, #blocked2> loc(#loc246)
    %V_load_170 = arith.addi %V_load_166, %V_load_169 : tensor<64x64xi64, #blocked2> loc(#loc246)
    %V_load_171 = tt.addptr %value_cache_ptr, %v_offset_151 : !tt.ptr<bf16>, i64 loc(#loc246)
    %V_load_172 = arith.trunci %V_load_170 : tensor<64x64xi64, #blocked2> to tensor<64x64xi32, #blocked2> loc(#loc246)
    %V_load_173 = ttg.memdesc_index %V_load_145[%c0_i32] : !ttg.memdesc<3x64x64xbf16, #shared2, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 3x64x64> loc(#loc232)
    %acc_174 = tt.splat %acc : i1 -> tensor<64x64xi1, #blocked2> loc(#loc298)
    %acc_175 = arith.andi %acc_174, %V_load : tensor<64x64xi1, #blocked2> loc(#loc298)
    %V_load_176 = amdgpu.buffer_load_to_local %V_load_171[%V_load_172] mask = %acc_175 into %V_load_173 : <bf16>[tensor<64x64xi32, #blocked2>]  -> <64x64xbf16, #shared2, #smem, mutable, 3x64x64> loc(#loc232)
    %V_load_177 = ttg.async_commit_group tokens %V_load_176 loc(#loc232)
    %acc_178 = arith.cmpi sgt, %num_blocks_129, %c2_i32 : i32 loc(#loc298)
    %physical_block_idx_179 = tt.addptr %physical_block_idx, %c2_i32 : !tt.ptr<i32>, i32 loc(#loc240)
    %physical_block_idx_180 = tt.load %physical_block_idx_179, %acc_178 {amd.pipeliner_part = "prologue"} : !tt.ptr<i32> loc(#loc225)
    %v_offset_181 = arith.extsi %physical_block_idx_148 : i32 to i64 loc(#loc241)
    %v_offset_182 = arith.muli %v_offset_181, %stride_v_cache_0 : i64 loc(#loc241)
    %v_offset_183 = arith.addi %v_offset_182, %v_offset_131 : i64 loc(#loc242)
    %k_offset_184 = arith.muli %v_offset_181, %stride_k_cache_0 : i64 loc(#loc243)
    %k_offset_185 = arith.addi %k_offset_184, %k_offset : i64 loc(#loc244)
    %K_load_186 = tt.addptr %key_cache_ptr, %k_offset_185 : !tt.ptr<bf16>, i64 loc(#loc245)
    %K_load_187 = ttg.memdesc_index %K_load_144[%c1_i32] : !ttg.memdesc<3x64x64xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable, 3x64x64> loc(#loc231)
    %acc_188 = tt.splat %acc_146 : i1 -> tensor<64x64xi1, #blocked1> loc(#loc298)
    %acc_189 = arith.andi %acc_188, %K_load_137 : tensor<64x64xi1, #blocked1> loc(#loc298)
    %K_load_190 = amdgpu.buffer_load_to_local %K_load_186[%K_load_160] mask = %acc_189 into %K_load_187 : <bf16>[tensor<64x64xi32, #blocked1>]  -> <64x64xbf16, #shared1, #smem, mutable, 3x64x64> loc(#loc231)
    %K_load_191 = ttg.async_commit_group tokens %K_load_190 loc(#loc231)
    %V_load_192 = tt.addptr %value_cache_ptr, %v_offset_183 : !tt.ptr<bf16>, i64 loc(#loc246)
    %V_load_193 = ttg.memdesc_index %V_load_145[%c1_i32] : !ttg.memdesc<3x64x64xbf16, #shared2, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 3x64x64> loc(#loc232)
    %acc_194 = tt.splat %acc_146 : i1 -> tensor<64x64xi1, #blocked2> loc(#loc298)
    %acc_195 = arith.andi %acc_194, %V_load : tensor<64x64xi1, #blocked2> loc(#loc298)
    %V_load_196 = amdgpu.buffer_load_to_local %V_load_192[%V_load_172] mask = %acc_195 into %V_load_193 : <bf16>[tensor<64x64xi32, #blocked2>]  -> <64x64xbf16, #shared2, #smem, mutable, 3x64x64> loc(#loc232)
    %V_load_197 = ttg.async_commit_group tokens %V_load_196 loc(#loc232)
    %acc_198 = arith.subi %num_blocks_129, %c3_i32 : i32 loc(#loc298)
    %acc_199:13 = scf.for %acc_337 = %c0_i32 to %acc_198 step %c1_i32 iter_args(%M_338 = %M_125, %arg28 = %cst_13, %arg29 = %cst_15, %acc_339 = %c1_i32, %K_load_340 = %K_load_165, %K_load_341 = %K_load_191, %V_load_342 = %V_load_177, %V_load_343 = %V_load_197, %physical_block_idx_344 = %physical_block_idx_180, %K_load_345 = %K_load_161, %K_load_346 = %K_load_187, %V_load_347 = %V_load_173, %V_load_348 = %V_load_193) -> (tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128x64xf32, #mma1>, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, i32, !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable, 3x64x64>, !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable, 3x64x64>, !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 3x64x64>, !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 3x64x64>)  : i32 {
      %K_load_349 = ttg.async_wait %K_load_340, %V_load_342 {num = 5 : i32} loc(#loc231)
      %acc_350 = arith.addi %acc_339, %c1_i32 : i32 loc(#loc298)
      %acc_351 = arith.cmpi slt, %acc_350, %c3_i32 : i32 loc(#loc298)
      %acc_352 = arith.select %acc_351, %acc_350, %c0_i32 : i32 loc(#loc298)
      %acc_353 = arith.addi %acc_337, %c3_i32 : i32 loc(#loc298)
      %physical_block_idx_354 = tt.addptr %physical_block_idx, %acc_353 : !tt.ptr<i32>, i32 loc(#loc240)
      %physical_block_idx_355 = tt.load %physical_block_idx_354 : !tt.ptr<i32> loc(#loc225)
      %v_offset_356 = arith.extsi %physical_block_idx_344 : i32 to i64 loc(#loc241)
      %v_offset_357 = arith.muli %v_offset_356, %stride_v_cache_0 : i64 loc(#loc241)
      %v_offset_358 = arith.addi %v_offset_357, %v_offset_131 : i64 loc(#loc242)
      %k_offset_359 = arith.muli %v_offset_356, %stride_k_cache_0 : i64 loc(#loc243)
      %k_offset_360 = arith.addi %k_offset_359, %k_offset : i64 loc(#loc244)
      %K_load_361 = tt.addptr %key_cache_ptr, %k_offset_360 : !tt.ptr<bf16>, i64 loc(#loc245)
      %K_load_362 = ttg.memdesc_index %K_load_144[%acc_352] : !ttg.memdesc<3x64x64xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable, 3x64x64> loc(#loc231)
      %K_load_363 = amdgpu.buffer_load_to_local %K_load_361[%K_load_160] mask = %K_load_137 into %K_load_362 : <bf16>[tensor<64x64xi32, #blocked1>]  -> <64x64xbf16, #shared1, #smem, mutable, 3x64x64> loc(#loc231)
      %K_load_364 = ttg.async_commit_group tokens %K_load_363 loc(#loc231)
      %K_load_365 = ttg.local_load %K_load_345 token %K_load_349 : !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable, 3x64x64> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc231)
      %V_load_366 = tt.addptr %value_cache_ptr, %v_offset_358 : !tt.ptr<bf16>, i64 loc(#loc246)
      %V_load_367 = ttg.memdesc_index %V_load_145[%acc_352] : !ttg.memdesc<3x64x64xbf16, #shared2, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 3x64x64> loc(#loc232)
      %V_load_368 = amdgpu.buffer_load_to_local %V_load_366[%V_load_172] mask = %V_load into %V_load_367 : <bf16>[tensor<64x64xi32, #blocked2>]  -> <64x64xbf16, #shared2, #smem, mutable, 3x64x64> loc(#loc232)
      %V_load_369 = ttg.async_commit_group tokens %V_load_368 loc(#loc232)
      %V_load_370 = ttg.local_load %V_load_347 token %K_load_349 : !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 3x64x64> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma1, kWidth = 4}>> loc(#loc232)
      %seq_offset_371 = arith.muli %acc_337, %c64_i32 : i32 loc(#loc247)
      %seq_offset_372 = tt.splat %seq_offset_371 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc248)
      %seq_offset_373 = arith.addi %seq_offset_372, %offs_d_35 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc248)
      %seq_mask_374 = tt.expand_dims %seq_offset_373 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x64xi32, #mma> loc(#loc249)
      %seq_mask_375 = tt.broadcast %seq_mask_374 : tensor<1x64xi32, #mma> -> tensor<128x64xi32, #mma> loc(#loc236)
      %seq_mask_376 = arith.cmpi slt, %seq_mask_375, %seq_mask_141 : tensor<128x64xi32, #mma> loc(#loc236)
      %S_377 = tt.dot %Q_122, %K_load_365, %cst_16 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc250)
      %S_378 = arith.mulf %S, %S_377 : tensor<128x64xf32, #mma> loc(#loc237)
      %S_379 = arith.addf %S_378, %cst_16 : tensor<128x64xf32, #mma> loc(#loc251)
      %S_380 = arith.andi %S_143, %seq_mask_376 : tensor<128x64xi1, #mma> loc(#loc239)
      %S_381 = arith.select %S_380, %S_379, %cst_12 : tensor<128x64xi1, #mma>, tensor<128x64xf32, #mma> loc(#loc252)
      %m_j_382 = "tt.reduce"(%S_381) <{axis = 1 : i32}> ({
      ^bb0(%m_j_403: f32 loc(callsite(#loc1 at #loc253)), %m_j_404: f32 loc(callsite(#loc1 at #loc253))):
        %m_j_405 = arith.maxnumf %m_j_403, %m_j_404 : f32 loc(#loc299)
        tt.reduce.return %m_j_405 : f32 loc(#loc291)
      }) : (tensor<128x64xf32, #mma>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc291)
      %m_j_383 = arith.maxnumf %M_338, %m_j_382 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc254)
      %m_j_384 = arith.cmpf ogt, %m_j_383, %cst_14 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc255)
      %m_j_385 = arith.select %m_j_384, %m_j_383, %cst_11 : tensor<128xi1, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc256)
      %P_386 = tt.expand_dims %m_j_385 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc257)
      %P_387 = tt.broadcast %P_386 : tensor<128x1xf32, #mma> -> tensor<128x64xf32, #mma> loc(#loc258)
      %P_388 = arith.subf %S_381, %P_387 : tensor<128x64xf32, #mma> loc(#loc258)
      %P_389 = math.exp %P_388 : tensor<128x64xf32, #mma> loc(#loc259)
      %l_j_390 = "tt.reduce"(%P_389) <{axis = 1 : i32}> ({
      ^bb0(%l_j_403: f32 loc(callsite(#loc1 at #loc260)), %l_j_404: f32 loc(callsite(#loc1 at #loc260))):
        %l_j_405 = arith.addf %l_j_403, %l_j_404 : f32 loc(#loc300)
        tt.reduce.return %l_j_405 : f32 loc(#loc293)
      }) : (tensor<128x64xf32, #mma>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc293)
      %alpha_391 = arith.subf %M_338, %m_j_385 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc261)
      %alpha_392 = math.exp %alpha_391 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc262)
      %acc_393 = tt.expand_dims %alpha_392 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc263)
      %acc_394 = ttg.convert_layout %acc_393 : tensor<128x1xf32, #mma> -> tensor<128x1xf32, #mma1> loc(#loc264)
      %acc_395 = tt.broadcast %acc_394 : tensor<128x1xf32, #mma1> -> tensor<128x64xf32, #mma1> loc(#loc264)
      %acc_396 = arith.mulf %arg29, %acc_395 : tensor<128x64xf32, #mma1> loc(#loc264)
      %L_397 = arith.mulf %arg28, %alpha_392 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc265)
      %L_398 = arith.addf %L_397, %l_j_390 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc266)
      %acc_399 = arith.truncf %P_389 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc267)
      %acc_400 = ttg.local_alloc %acc_399 : (tensor<128x64xbf16, #mma>) -> !ttg.memdesc<128x64xbf16, #shared3, #smem> loc(#loc267)
      %acc_401 = ttg.local_load %acc_400 : !ttg.memdesc<128x64xbf16, #shared3, #smem> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 4}>> loc(#loc267)
      %acc_402 = tt.dot %acc_401, %V_load_370, %acc_396 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 4}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma1, kWidth = 4}>> -> tensor<128x64xf32, #mma1> loc(#loc268)
      scf.yield %m_j_385, %L_398, %acc_402, %acc_352, %K_load_341, %K_load_364, %V_load_343, %V_load_369, %physical_block_idx_355, %K_load_346, %K_load_362, %V_load_348, %V_load_367 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128x64xf32, #mma1>, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, i32, !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable, 3x64x64>, !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable, 3x64x64>, !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 3x64x64>, !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 3x64x64> loc(#loc298)
    } loc(#loc298)
    %acc_200 = arith.maxsi %acc_198, %c0_i32 : i32 loc(#loc298)
    %acc_201 = arith.addi %acc_200, %c1_i32 : i32 loc(#loc298)
    %acc_202 = arith.cmpi sge, %num_blocks_129, %c1_i32 : i32 loc(#loc298)
    %acc_203 = arith.addi %acc_200, %c2_i32 : i32 loc(#loc298)
    %acc_204 = arith.cmpi sge, %num_blocks_129, %c2_i32 : i32 loc(#loc298)
    %acc_205 = arith.cmpi sge, %num_blocks_129, %c3_i32 : i32 loc(#loc298)
    %K_load_206 = ttg.async_wait %acc_199#4, %acc_199#6 {num = 0 : i32} loc(#loc231)
    %acc_207 = arith.addi %acc_199#3, %c1_i32 : i32 loc(#loc298)
    %acc_208 = arith.cmpi slt, %acc_207, %c3_i32 : i32 loc(#loc298)
    %acc_209 = arith.select %acc_208, %acc_207, %c0_i32 : i32 loc(#loc298)
    %v_offset_210 = arith.extsi %acc_199#8 : i32 to i64 loc(#loc241)
    %v_offset_211 = arith.muli %v_offset_210, %stride_v_cache_0 : i64 loc(#loc241)
    %v_offset_212 = arith.addi %v_offset_211, %v_offset_131 : i64 loc(#loc242)
    %k_offset_213 = arith.muli %v_offset_210, %stride_k_cache_0 : i64 loc(#loc243)
    %k_offset_214 = arith.addi %k_offset_213, %k_offset : i64 loc(#loc244)
    %K_load_215 = tt.addptr %key_cache_ptr, %k_offset_214 : !tt.ptr<bf16>, i64 loc(#loc245)
    %K_load_216 = ttg.memdesc_index %K_load_144[%acc_209] : !ttg.memdesc<3x64x64xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable, 3x64x64> loc(#loc231)
    %acc_217 = tt.splat %acc_205 : i1 -> tensor<64x64xi1, #blocked1> loc(#loc298)
    %acc_218 = arith.andi %acc_217, %K_load_137 : tensor<64x64xi1, #blocked1> loc(#loc298)
    %K_load_219 = amdgpu.buffer_load_to_local %K_load_215[%K_load_160] mask = %acc_218 into %K_load_216 : <bf16>[tensor<64x64xi32, #blocked1>]  -> <64x64xbf16, #shared1, #smem, mutable, 3x64x64> loc(#loc231)
    %K_load_220 = ttg.async_commit_group tokens %K_load_219 loc(#loc231)
    %K_load_221 = ttg.local_load %acc_199#9 token %K_load_206 : !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable, 3x64x64> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc231)
    %V_load_222 = tt.addptr %value_cache_ptr, %v_offset_212 : !tt.ptr<bf16>, i64 loc(#loc246)
    %V_load_223 = ttg.memdesc_index %V_load_145[%acc_209] : !ttg.memdesc<3x64x64xbf16, #shared2, #smem, mutable> -> !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 3x64x64> loc(#loc232)
    %acc_224 = tt.splat %acc_205 : i1 -> tensor<64x64xi1, #blocked2> loc(#loc298)
    %acc_225 = arith.andi %acc_224, %V_load : tensor<64x64xi1, #blocked2> loc(#loc298)
    %V_load_226 = amdgpu.buffer_load_to_local %V_load_222[%V_load_172] mask = %acc_225 into %V_load_223 : <bf16>[tensor<64x64xi32, #blocked2>]  -> <64x64xbf16, #shared2, #smem, mutable, 3x64x64> loc(#loc232)
    %V_load_227 = ttg.async_commit_group tokens %V_load_226 loc(#loc232)
    %V_load_228 = ttg.local_load %acc_199#11 token %K_load_206 : !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 3x64x64> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma1, kWidth = 4}>> loc(#loc232)
    %seq_offset = arith.muli %acc_200, %c64_i32 : i32 loc(#loc247)
    %seq_offset_229 = tt.splat %seq_offset : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc248)
    %seq_offset_230 = arith.addi %seq_offset_229, %offs_d_35 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc248)
    %seq_mask_231 = tt.expand_dims %seq_offset_230 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x64xi32, #mma> loc(#loc249)
    %seq_mask_232 = tt.broadcast %seq_mask_231 : tensor<1x64xi32, #mma> -> tensor<128x64xi32, #mma> loc(#loc236)
    %seq_mask_233 = arith.cmpi slt, %seq_mask_232, %seq_mask_141 : tensor<128x64xi32, #mma> loc(#loc236)
    %S_234 = scf.if %acc_202 -> (tensor<128x64xf32, #mma>) {
      %S_337 = tt.dot %Q_122, %K_load_221, %cst_16 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc250)
      scf.yield %S_337 : tensor<128x64xf32, #mma> loc(#loc250)
    } else {
      scf.yield %cst_16 : tensor<128x64xf32, #mma> loc(#loc250)
    } loc(#loc250)
    %S_235 = arith.mulf %S, %S_234 : tensor<128x64xf32, #mma> loc(#loc237)
    %S_236 = arith.addf %S_235, %cst_16 : tensor<128x64xf32, #mma> loc(#loc251)
    %S_237 = arith.andi %S_143, %seq_mask_233 : tensor<128x64xi1, #mma> loc(#loc239)
    %S_238 = arith.select %S_237, %S_236, %cst_12 : tensor<128x64xi1, #mma>, tensor<128x64xf32, #mma> loc(#loc252)
    %m_j = "tt.reduce"(%S_238) <{axis = 1 : i32}> ({
    ^bb0(%m_j_337: f32 loc(callsite(#loc1 at #loc253)), %m_j_338: f32 loc(callsite(#loc1 at #loc253))):
      %m_j_339 = arith.maxnumf %m_j_337, %m_j_338 : f32 loc(#loc299)
      tt.reduce.return %m_j_339 : f32 loc(#loc291)
    }) : (tensor<128x64xf32, #mma>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc291)
    %m_j_239 = arith.maxnumf %acc_199#0, %m_j : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc254)
    %m_j_240 = arith.cmpf ogt, %m_j_239, %cst_14 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc255)
    %m_j_241 = arith.select %m_j_240, %m_j_239, %cst_11 : tensor<128xi1, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc256)
    %P = tt.expand_dims %m_j_241 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc257)
    %P_242 = tt.broadcast %P : tensor<128x1xf32, #mma> -> tensor<128x64xf32, #mma> loc(#loc258)
    %P_243 = arith.subf %S_238, %P_242 : tensor<128x64xf32, #mma> loc(#loc258)
    %P_244 = math.exp %P_243 : tensor<128x64xf32, #mma> loc(#loc259)
    %l_j = "tt.reduce"(%P_244) <{axis = 1 : i32}> ({
    ^bb0(%l_j_337: f32 loc(callsite(#loc1 at #loc260)), %l_j_338: f32 loc(callsite(#loc1 at #loc260))):
      %l_j_339 = arith.addf %l_j_337, %l_j_338 : f32 loc(#loc300)
      tt.reduce.return %l_j_339 : f32 loc(#loc293)
    }) : (tensor<128x64xf32, #mma>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc293)
    %alpha = arith.subf %acc_199#0, %m_j_241 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc261)
    %alpha_245 = math.exp %alpha : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc262)
    %acc_246 = tt.expand_dims %alpha_245 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc263)
    %acc_247 = ttg.convert_layout %acc_246 : tensor<128x1xf32, #mma> -> tensor<128x1xf32, #mma1> loc(#loc264)
    %acc_248 = tt.broadcast %acc_247 : tensor<128x1xf32, #mma1> -> tensor<128x64xf32, #mma1> loc(#loc264)
    %acc_249 = arith.mulf %acc_199#2, %acc_248 : tensor<128x64xf32, #mma1> loc(#loc264)
    %L = arith.mulf %acc_199#1, %alpha_245 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc265)
    %L_250 = arith.addf %L, %l_j : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc266)
    %acc_251 = arith.truncf %P_244 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc267)
    %acc_252 = ttg.local_alloc %acc_251 : (tensor<128x64xbf16, #mma>) -> !ttg.memdesc<128x64xbf16, #shared3, #smem> loc(#loc267)
    %acc_253 = ttg.local_load %acc_252 : !ttg.memdesc<128x64xbf16, #shared3, #smem> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 4}>> loc(#loc267)
    %acc_254 = scf.if %acc_202 -> (tensor<128x64xf32, #mma1>) {
      %acc_337 = tt.dot %acc_253, %V_load_228, %acc_249 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 4}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma1, kWidth = 4}>> -> tensor<128x64xf32, #mma1> loc(#loc268)
      scf.yield %acc_337 : tensor<128x64xf32, #mma1> loc(#loc268)
    } else {
      scf.yield %acc_249 : tensor<128x64xf32, #mma1> loc(#loc268)
    } loc(#loc268)
    %acc_255 = arith.select %acc_202, %m_j_241, %acc_199#0 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc298)
    %acc_256 = arith.select %acc_202, %L_250, %acc_199#1 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc298)
    %acc_257 = arith.select %acc_202, %acc_254, %acc_199#2 : tensor<128x64xf32, #mma1> loc(#loc298)
    %K_load_258 = ttg.async_wait %acc_199#5, %acc_199#7 {num = 5 : i32} loc(#loc231)
    %K_load_259 = ttg.local_load %acc_199#10 token %K_load_258 : !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable, 3x64x64> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc231)
    %V_load_260 = ttg.local_load %acc_199#12 token %K_load_258 : !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 3x64x64> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma1, kWidth = 4}>> loc(#loc232)
    %seq_offset_261 = arith.muli %acc_201, %c64_i32 : i32 loc(#loc247)
    %seq_offset_262 = tt.splat %seq_offset_261 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc248)
    %seq_offset_263 = arith.addi %seq_offset_262, %offs_d_35 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc248)
    %seq_mask_264 = tt.expand_dims %seq_offset_263 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x64xi32, #mma> loc(#loc249)
    %seq_mask_265 = tt.broadcast %seq_mask_264 : tensor<1x64xi32, #mma> -> tensor<128x64xi32, #mma> loc(#loc236)
    %seq_mask_266 = arith.cmpi slt, %seq_mask_265, %seq_mask_141 : tensor<128x64xi32, #mma> loc(#loc236)
    %S_267 = scf.if %acc_204 -> (tensor<128x64xf32, #mma>) {
      %S_337 = tt.dot %Q_122, %K_load_259, %cst_16 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc250)
      scf.yield %S_337 : tensor<128x64xf32, #mma> loc(#loc250)
    } else {
      scf.yield %cst_16 : tensor<128x64xf32, #mma> loc(#loc250)
    } loc(#loc250)
    %S_268 = arith.mulf %S, %S_267 : tensor<128x64xf32, #mma> loc(#loc237)
    %S_269 = arith.addf %S_268, %cst_16 : tensor<128x64xf32, #mma> loc(#loc251)
    %S_270 = arith.andi %S_143, %seq_mask_266 : tensor<128x64xi1, #mma> loc(#loc239)
    %S_271 = arith.select %S_270, %S_269, %cst_12 : tensor<128x64xi1, #mma>, tensor<128x64xf32, #mma> loc(#loc252)
    %m_j_272 = "tt.reduce"(%S_271) <{axis = 1 : i32}> ({
    ^bb0(%m_j_337: f32 loc(callsite(#loc1 at #loc253)), %m_j_338: f32 loc(callsite(#loc1 at #loc253))):
      %m_j_339 = arith.maxnumf %m_j_337, %m_j_338 : f32 loc(#loc299)
      tt.reduce.return %m_j_339 : f32 loc(#loc291)
    }) : (tensor<128x64xf32, #mma>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc291)
    %m_j_273 = arith.maxnumf %acc_255, %m_j_272 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc254)
    %m_j_274 = arith.cmpf ogt, %m_j_273, %cst_14 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc255)
    %m_j_275 = arith.select %m_j_274, %m_j_273, %cst_11 : tensor<128xi1, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc256)
    %P_276 = tt.expand_dims %m_j_275 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc257)
    %P_277 = tt.broadcast %P_276 : tensor<128x1xf32, #mma> -> tensor<128x64xf32, #mma> loc(#loc258)
    %P_278 = arith.subf %S_271, %P_277 : tensor<128x64xf32, #mma> loc(#loc258)
    %P_279 = math.exp %P_278 : tensor<128x64xf32, #mma> loc(#loc259)
    %l_j_280 = "tt.reduce"(%P_279) <{axis = 1 : i32}> ({
    ^bb0(%l_j_337: f32 loc(callsite(#loc1 at #loc260)), %l_j_338: f32 loc(callsite(#loc1 at #loc260))):
      %l_j_339 = arith.addf %l_j_337, %l_j_338 : f32 loc(#loc300)
      tt.reduce.return %l_j_339 : f32 loc(#loc293)
    }) : (tensor<128x64xf32, #mma>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc293)
    %alpha_281 = arith.subf %acc_255, %m_j_275 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc261)
    %alpha_282 = math.exp %alpha_281 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc262)
    %acc_283 = tt.expand_dims %alpha_282 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc263)
    %acc_284 = ttg.convert_layout %acc_283 : tensor<128x1xf32, #mma> -> tensor<128x1xf32, #mma1> loc(#loc264)
    %acc_285 = tt.broadcast %acc_284 : tensor<128x1xf32, #mma1> -> tensor<128x64xf32, #mma1> loc(#loc264)
    %acc_286 = arith.mulf %acc_257, %acc_285 : tensor<128x64xf32, #mma1> loc(#loc264)
    %L_287 = arith.mulf %acc_256, %alpha_282 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc265)
    %L_288 = arith.addf %L_287, %l_j_280 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc266)
    %acc_289 = arith.truncf %P_279 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc267)
    %acc_290 = ttg.local_alloc %acc_289 : (tensor<128x64xbf16, #mma>) -> !ttg.memdesc<128x64xbf16, #shared3, #smem> loc(#loc267)
    %acc_291 = ttg.local_load %acc_290 : !ttg.memdesc<128x64xbf16, #shared3, #smem> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 4}>> loc(#loc267)
    %acc_292 = scf.if %acc_204 -> (tensor<128x64xf32, #mma1>) {
      %acc_337 = tt.dot %acc_291, %V_load_260, %acc_286 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 4}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma1, kWidth = 4}>> -> tensor<128x64xf32, #mma1> loc(#loc268)
      scf.yield %acc_337 : tensor<128x64xf32, #mma1> loc(#loc268)
    } else {
      scf.yield %acc_286 : tensor<128x64xf32, #mma1> loc(#loc268)
    } loc(#loc268)
    %acc_293 = arith.select %acc_204, %m_j_275, %acc_255 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc298)
    %acc_294 = arith.select %acc_204, %L_288, %acc_256 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc298)
    %acc_295 = arith.select %acc_204, %acc_292, %acc_257 : tensor<128x64xf32, #mma1> loc(#loc298)
    %K_load_296 = ttg.async_wait %K_load_220, %V_load_227 {num = 0 : i32} loc(#loc231)
    %K_load_297 = ttg.local_load %K_load_216 token %K_load_296 : !ttg.memdesc<64x64xbf16, #shared1, #smem, mutable, 3x64x64> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc231)
    %V_load_298 = ttg.local_load %V_load_223 token %K_load_296 : !ttg.memdesc<64x64xbf16, #shared2, #smem, mutable, 3x64x64> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma1, kWidth = 4}>> loc(#loc232)
    %seq_offset_299 = arith.muli %acc_203, %c64_i32 : i32 loc(#loc247)
    %seq_offset_300 = tt.splat %seq_offset_299 : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc248)
    %seq_offset_301 = arith.addi %seq_offset_300, %offs_d_35 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc248)
    %seq_mask_302 = tt.expand_dims %seq_offset_301 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x64xi32, #mma> loc(#loc249)
    %seq_mask_303 = tt.broadcast %seq_mask_302 : tensor<1x64xi32, #mma> -> tensor<128x64xi32, #mma> loc(#loc236)
    %seq_mask_304 = arith.cmpi slt, %seq_mask_303, %seq_mask_141 : tensor<128x64xi32, #mma> loc(#loc236)
    %S_305 = scf.if %acc_205 -> (tensor<128x64xf32, #mma>) {
      %S_337 = tt.dot %Q_122, %K_load_297, %cst_16 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc250)
      scf.yield %S_337 : tensor<128x64xf32, #mma> loc(#loc250)
    } else {
      scf.yield %cst_16 : tensor<128x64xf32, #mma> loc(#loc250)
    } loc(#loc250)
    %S_306 = arith.mulf %S, %S_305 : tensor<128x64xf32, #mma> loc(#loc237)
    %S_307 = arith.addf %S_306, %cst_16 : tensor<128x64xf32, #mma> loc(#loc251)
    %S_308 = arith.andi %S_143, %seq_mask_304 : tensor<128x64xi1, #mma> loc(#loc239)
    %S_309 = arith.select %S_308, %S_307, %cst_12 : tensor<128x64xi1, #mma>, tensor<128x64xf32, #mma> loc(#loc252)
    %m_j_310 = "tt.reduce"(%S_309) <{axis = 1 : i32}> ({
    ^bb0(%m_j_337: f32 loc(callsite(#loc1 at #loc253)), %m_j_338: f32 loc(callsite(#loc1 at #loc253))):
      %m_j_339 = arith.maxnumf %m_j_337, %m_j_338 : f32 loc(#loc299)
      tt.reduce.return %m_j_339 : f32 loc(#loc291)
    }) : (tensor<128x64xf32, #mma>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc291)
    %m_j_311 = arith.maxnumf %acc_293, %m_j_310 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc254)
    %m_j_312 = arith.cmpf ogt, %m_j_311, %cst_14 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc255)
    %m_j_313 = arith.select %m_j_312, %m_j_311, %cst_11 : tensor<128xi1, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc256)
    %P_314 = tt.expand_dims %m_j_313 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc257)
    %P_315 = tt.broadcast %P_314 : tensor<128x1xf32, #mma> -> tensor<128x64xf32, #mma> loc(#loc258)
    %P_316 = arith.subf %S_309, %P_315 : tensor<128x64xf32, #mma> loc(#loc258)
    %P_317 = math.exp %P_316 : tensor<128x64xf32, #mma> loc(#loc259)
    %l_j_318 = "tt.reduce"(%P_317) <{axis = 1 : i32}> ({
    ^bb0(%l_j_337: f32 loc(callsite(#loc1 at #loc260)), %l_j_338: f32 loc(callsite(#loc1 at #loc260))):
      %l_j_339 = arith.addf %l_j_337, %l_j_338 : f32 loc(#loc300)
      tt.reduce.return %l_j_339 : f32 loc(#loc293)
    }) : (tensor<128x64xf32, #mma>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc293)
    %alpha_319 = arith.subf %acc_293, %m_j_313 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc261)
    %alpha_320 = math.exp %alpha_319 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc262)
    %acc_321 = tt.expand_dims %alpha_320 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc263)
    %acc_322 = ttg.convert_layout %acc_321 : tensor<128x1xf32, #mma> -> tensor<128x1xf32, #mma1> loc(#loc264)
    %acc_323 = tt.broadcast %acc_322 : tensor<128x1xf32, #mma1> -> tensor<128x64xf32, #mma1> loc(#loc264)
    %acc_324 = arith.mulf %acc_295, %acc_323 : tensor<128x64xf32, #mma1> loc(#loc264)
    %L_325 = arith.mulf %acc_294, %alpha_320 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc265)
    %L_326 = arith.addf %L_325, %l_j_318 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc266)
    %acc_327 = arith.truncf %P_317 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc267)
    %acc_328 = ttg.local_alloc %acc_327 : (tensor<128x64xbf16, #mma>) -> !ttg.memdesc<128x64xbf16, #shared3, #smem> loc(#loc267)
    %acc_329 = ttg.local_load %acc_328 : !ttg.memdesc<128x64xbf16, #shared3, #smem> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 4}>> loc(#loc267)
    %acc_330 = scf.if %acc_205 -> (tensor<128x64xf32, #mma1>) {
      %acc_337 = tt.dot %acc_329, %V_load_298, %acc_324 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 4}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma1, kWidth = 4}>> -> tensor<128x64xf32, #mma1> loc(#loc268)
      scf.yield %acc_337 : tensor<128x64xf32, #mma1> loc(#loc268)
    } else {
      scf.yield %acc_324 : tensor<128x64xf32, #mma1> loc(#loc268)
    } loc(#loc268)
    %acc_331 = arith.select %acc_205, %L_326, %acc_294 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc298)
    %acc_332 = arith.select %acc_205, %acc_330, %acc_295 : tensor<128x64xf32, #mma1> loc(#loc298)
    ttg.local_dealloc %V_load_145 : !ttg.memdesc<3x64x64xbf16, #shared2, #smem, mutable> loc(#loc298)
    ttg.local_dealloc %K_load_144 : !ttg.memdesc<3x64x64xbf16, #shared1, #smem, mutable> loc(#loc298)
    %one_over_L = tt.expand_dims %acc_331 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc269)
    %one_over_L_333 = arith.divf %cst_10, %one_over_L : tensor<128x1xf32, #mma> loc(#loc270)
    %acc_334 = ttg.convert_layout %one_over_L_333 : tensor<128x1xf32, #mma> -> tensor<128x1xf32, #mma1> loc(#loc271)
    %acc_335 = tt.broadcast %acc_334 : tensor<128x1xf32, #mma1> -> tensor<128x64xf32, #mma1> loc(#loc271)
    %acc_336 = arith.mulf %acc_332, %acc_335 : tensor<128x64xf32, #mma1> loc(#loc271)
    %2 = tt.splat %output_stride_0 : i64 -> tensor<128x1xi64, #linear> loc(#loc136)
    %3 = arith.muli %query_offset_58, %2 : tensor<128x1xi64, #linear> loc(#loc136)
    %4 = tt.splat %output_stride_1 : i64 -> tensor<128x1xi64, #linear> loc(#loc136)
    %5 = arith.muli %query_offset_62, %4 : tensor<128x1xi64, #linear> loc(#loc136)
    %6 = arith.addi %3, %5 : tensor<128x1xi64, #linear> loc(#loc136)
    %7 = tt.broadcast %6 : tensor<128x1xi64, #linear> -> tensor<128x64xi64, #linear> loc(#loc136)
    %8 = tt.broadcast %query_offset_70 : tensor<1x64xi64, #linear> -> tensor<128x64xi64, #linear> loc(#loc136)
    %9 = arith.addi %7, %8 : tensor<128x64xi64, #linear> loc(#loc136)
    %10 = arith.trunci %9 : tensor<128x64xi64, #linear> to tensor<128x64xi32, #linear> loc(#loc136)
    %11 = arith.truncf %acc_336 : tensor<128x64xf32, #mma1> to tensor<128x64xbf16, #mma1> loc(#loc137)
    %12 = ttg.convert_layout %11 : tensor<128x64xbf16, #mma1> -> tensor<128x64xbf16, #linear> loc(#loc137)
    %13 = tt.splat %output_ptr : !tt.ptr<bf16> -> tensor<128x64x!tt.ptr<bf16>, #linear> loc(#loc137)
    %14 = tt.addptr %13, %10 : tensor<128x64x!tt.ptr<bf16>, #linear>, tensor<128x64xi32, #linear> loc(#loc137)
    tt.store %14, %12, %Q_107 : tensor<128x64x!tt.ptr<bf16>, #linear> loc(#loc137)
    tt.return loc(#loc138)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":98:32)
#loc3 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":99:39)
#loc4 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":101:14)
#loc5 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":102:14)
#loc6 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":103:14)
#loc7 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":104:14)
#loc8 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":105:14)
#loc9 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":106:14)
#loc10 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":107:14)
#loc11 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":108:14)
#loc12 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":109:14)
#loc13 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":110:14)
#loc14 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":111:14)
#loc15 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":112:14)
#loc16 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":113:14)
#loc17 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":36:4)
#loc19 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":36:17)
#loc22 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":37:22)
#loc23 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":37:32)
#loc24 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":38:44)
#loc25 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":38:22)
#loc26 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":39:25)
#loc27 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":39:35)
#loc28 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":41:22)
#loc29 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":41:11)
#loc30 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":42:25)
#loc31 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":41:8)
#loc32 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":46:18)
#loc33 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":119:54)
#loc34 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":119:32)
#loc35 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":119:66)
#loc36 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":119:76)
#loc37 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":121:45)
#loc38 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":124:74)
#loc39 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":124:42)
#loc40 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":126:56)
#loc41 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":128:27)
#loc42 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":128:38)
#loc43 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":129:8)
#loc44 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":131:26)
#loc45 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":132:26)
#loc46 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":133:56)
#loc47 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":133:46)
#loc48 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":135:52)
#loc49 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":136:35)
#loc50 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":136:65)
#loc51 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":136:56)
#loc52 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":138:23)
#loc53 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":138:34)
#loc54 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":139:25)
#loc55 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":139:36)
#loc56 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":140:17)
#loc57 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":140:10)
#loc58 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":143:33)
#loc59 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":143:47)
#loc60 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":143:53)
#loc61 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":144:31)
#loc62 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":145:36)
#loc63 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":155:22)
#loc64 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":155:46)
#loc65 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":155:33)
#loc66 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":155:70)
#loc67 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":155:57)
#loc68 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":154:20)
#loc69 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":154:8)
#loc70 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":160:35)
#loc71 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":166:23)
#loc72 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":166:12)
#loc73 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":169:13)
#loc74 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":175:37)
#loc75 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":175:22)
#loc76 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":178:28)
#loc77 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":196:10)
#loc78 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":198:10)
#loc79 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":203:56)
#loc80 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":15:20)
#loc81 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":208:45)
#loc82 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":15:26)
#loc83 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":220:56)
#loc84 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":218:37)
#loc85 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":220:37)
#loc86 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":226:28)
#loc87 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":228:21)
#loc88 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":228:32)
#loc89 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":233:28)
#loc90 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":241:26)
#loc91 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":240:12)
#loc92 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":256:12)
#loc93 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":272:65)
#loc94 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":272:55)
#loc95 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":272:76)
#loc96 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":272:41)
#loc97 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":277:21)
#loc98 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":283:36)
#loc99 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":283:60)
#loc100 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":220:77)
#loc101 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":225:33)
#loc102 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":226:14)
#loc103 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":232:33)
#loc104 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":233:14)
#loc105 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":240:28)
#loc106 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":256:30)
#loc107 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":270:25)
#loc108 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":270:38)
#loc109 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":272:30)
#loc110 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":277:31)
#loc111 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":277:13)
#loc112 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":283:73)
#loc113 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/triton/language/standard.py":189:40)
#loc115 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/triton/language/standard.py":168:27)
#loc116 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":310:28)
#loc117 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":313:29)
#loc118 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":313:49)
#loc119 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":316:27)
#loc120 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":316:23)
#loc121 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":316:19)
#loc122 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/triton/language/standard.py":291:36)
#loc124 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/triton/language/standard.py":261:15)
#loc125 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":322:27)
#loc126 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":322:23)
#loc127 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":325:26)
#loc128 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":325:20)
#loc129 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":328:16)
#loc130 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":328:24)
#loc131 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":332:27)
#loc132 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":332:37)
#loc133 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":335:25)
#loc134 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":335:23)
#loc135 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":336:16)
#loc136 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":348:21)
#loc137 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":349:8)
#loc138 = loc("/root/code/triton-s-load/unified_attn_ubench/unified_attention_aiter.py":347:4)
#loc165 = loc("kv_head_idx"(#loc2))
#loc166 = loc("q_block_global_idx"(#loc3))
#loc167 = loc("left"(#loc17))
#loc171 = loc("mid"(#loc22))
#loc172 = loc("mid"(#loc23))
#loc173 = loc("val"(#loc24))
#loc174 = loc("val"(#loc25))
#loc175 = loc("mid_val"(#loc26))
#loc176 = loc("mid_val"(#loc27))
#loc177 = loc("left"(#loc30))
#loc178 = loc("q_block_start_idx"(#loc33))
#loc179 = loc("q_block_start_idx"(#loc34))
#loc180 = loc("q_block_start_idx"(#loc35))
#loc181 = loc("q_block_start_idx"(#loc36))
#loc182 = loc("q_block_local_idx"(#loc37))
#loc183 = loc("cur_batch_in_all_stop_index"(#loc38))
#loc184 = loc("cur_batch_in_all_stop_index"(#loc39))
#loc185 = loc("cur_batch_query_len"(#loc40))
#loc186 = loc("offs_m"(#loc44))
#loc187 = loc("offs_d"(#loc45))
#loc188 = loc("query_pos"(#loc46))
#loc189 = loc("query_pos"(#loc47))
#loc190 = loc("query_offset_0"(#loc48))
#loc191 = loc("query_offset_1"(#loc49))
#loc192 = loc("query_offset_1"(#loc50))
#loc193 = loc("query_offset_1"(#loc51))
#loc194 = loc("query_offset"(#loc52))
#loc195 = loc("query_offset"(#loc53))
#loc196 = loc("query_offset"(#loc54))
#loc197 = loc("query_offset"(#loc55))
#loc198 = loc("query_offset"(#loc56))
#loc199 = loc("query_offset"(#loc57))
#loc200 = loc("dim_mask"(#loc58))
#loc201 = loc("dim_mask"(#loc59))
#loc202 = loc("dim_mask"(#loc60))
#loc203 = loc("query_mask_0"(#loc61))
#loc204 = loc("query_mask_1"(#loc62))
#loc205 = loc("Q"(#loc63))
#loc206 = loc("Q"(#loc64))
#loc207 = loc("Q"(#loc65))
#loc208 = loc("Q"(#loc66))
#loc209 = loc("Q"(#loc67))
#loc210 = loc("Q"(#loc68))
#loc211 = loc("Q"(#loc69))
#loc212 = loc("block_table_offset"(#loc70))
#loc213 = loc("M"(#loc71))
#loc214 = loc("M"(#loc72))
#loc215 = loc("M"(#loc73))
#loc216 = loc("seq_len"(#loc74))
#loc217 = loc("seq_len"(#loc75))
#loc218 = loc("context_len"(#loc76))
#loc219 = loc("max_seq_prefix_len"(#loc77))
#loc220 = loc("max_seq_prefix_len"(#loc78))
#loc221 = loc("max_seq_prefix_len"(#loc79))
#loc222 = loc("num_blocks"(#loc81))
#loc223 = loc("physical_block_idx"(#loc83))
#loc224 = loc("M"(#loc84))
#loc225 = loc("physical_block_idx"(#loc85))
#loc226 = loc("v_offset"(#loc86))
#loc227 = loc("v_offset"(#loc87))
#loc228 = loc("v_offset"(#loc88))
#loc229 = loc("k_offset"(#loc89))
#loc230 = loc("K_load"(#loc90))
#loc231 = loc("K_load"(#loc91))
#loc232 = loc("V_load"(#loc92))
#loc233 = loc("seq_mask"(#loc93))
#loc234 = loc("seq_mask"(#loc94))
#loc235 = loc("seq_mask"(#loc95))
#loc236 = loc("seq_mask"(#loc96))
#loc237 = loc("S"(#loc97))
#loc238 = loc("S"(#loc98))
#loc239 = loc("S"(#loc99))
#loc240 = loc("physical_block_idx"(#loc100))
#loc241 = loc("v_offset"(#loc101))
#loc242 = loc("v_offset"(#loc102))
#loc243 = loc("k_offset"(#loc103))
#loc244 = loc("k_offset"(#loc104))
#loc245 = loc("K_load"(#loc105))
#loc246 = loc("V_load"(#loc106))
#loc247 = loc("seq_offset"(#loc107))
#loc248 = loc("seq_offset"(#loc108))
#loc249 = loc("seq_mask"(#loc109))
#loc250 = loc("S"(#loc110))
#loc251 = loc("S"(#loc111))
#loc252 = loc("S"(#loc112))
#loc254 = loc("m_j"(#loc116))
#loc255 = loc("m_j"(#loc117))
#loc256 = loc("m_j"(#loc118))
#loc257 = loc("P"(#loc119))
#loc258 = loc("P"(#loc120))
#loc259 = loc("P"(#loc121))
#loc261 = loc("alpha"(#loc125))
#loc262 = loc("alpha"(#loc126))
#loc263 = loc("acc"(#loc127))
#loc264 = loc("acc"(#loc128))
#loc265 = loc("L"(#loc129))
#loc266 = loc("L"(#loc130))
#loc267 = loc("acc"(#loc131))
#loc268 = loc("acc"(#loc132))
#loc269 = loc("one_over_L"(#loc133))
#loc270 = loc("one_over_L"(#loc134))
#loc271 = loc("acc"(#loc135))
#loc272 = loc("right"(#loc167))
#loc273 = loc(callsite(#loc19 at #loc168))
#loc276 = loc(callsite(#loc171 at #loc168))
#loc277 = loc("right"(#loc172))
#loc278 = loc(callsite(#loc173 at #loc168))
#loc279 = loc(callsite(#loc174 at #loc168))
#loc280 = loc(callsite(#loc175 at #loc168))
#loc281 = loc(callsite(#loc176 at #loc168))
#loc282 = loc(callsite(#loc28 at #loc168))
#loc283 = loc(callsite(#loc29 at #loc168))
#loc284 = loc("left"(#loc177))
#loc285 = loc(callsite(#loc1 at #loc168))
#loc286 = loc(callsite(#loc31 at #loc168))
#loc287 = loc(callsite(#loc32 at #loc168))
#loc288 = loc(callsite(#loc80 at #loc222))
#loc289 = loc(callsite(#loc82 at #loc222))
#loc290 = loc("L"(#loc224))
#loc291 = loc(callsite(#loc113 at #loc253))
#loc293 = loc(callsite(#loc122 at #loc260))
#loc295 = loc(callsite(#loc272 at #loc168))
#loc296 = loc(callsite(#loc277 at #loc168))
#loc297 = loc(callsite(#loc284 at #loc168))
#loc298 = loc("acc"(#loc290))
#loc299 = loc(callsite(#loc115 at #loc291))
#loc300 = loc(callsite(#loc124 at #loc293))
