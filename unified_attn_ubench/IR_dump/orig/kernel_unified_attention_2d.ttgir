#blocked = #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>
#linear = #ttg.linear<{register = [[0, 1], [0, 2], [0, 4], [0, 16], [0, 32]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 8]], warp = [[32, 0], [64, 0]], block = []}>
#loc = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":50:0)
#loc1 = loc(unknown)
#loc2 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":116:68)
#loc20 = loc("left")
#loc21 = loc("right")
#loc114 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":310:35)
#loc123 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":319:21)
#mma = #ttg.amd_mfma<{version = 4, warpsPerCTA = [4, 1], instrShape = [32, 32], isTransposed = true}>
#shared = #ttg.swizzled_shared<{vec = 8, perPhase = 2, maxPhase = 8, order = [1, 0]}>
#shared1 = #ttg.swizzled_shared<{vec = 8, perPhase = 2, maxPhase = 8, order = [0, 1]}>
#shared2 = #ttg.swizzled_shared<{vec = 4, perPhase = 2, maxPhase = 8, order = [1, 0]}>
#smem = #ttg.shared_memory
#loc140 = loc("output_ptr"(#loc))
#loc141 = loc("query_ptr"(#loc))
#loc142 = loc("key_cache_ptr"(#loc))
#loc143 = loc("value_cache_ptr"(#loc))
#loc144 = loc("sink_ptr"(#loc))
#loc145 = loc("block_tables_ptr"(#loc))
#loc146 = loc("seq_lens_ptr"(#loc))
#loc147 = loc("scale"(#loc))
#loc148 = loc("k_scale"(#loc))
#loc149 = loc("v_scale"(#loc))
#loc150 = loc("out_scale"(#loc))
#loc151 = loc("softcap"(#loc))
#loc152 = loc("block_table_stride"(#loc))
#loc153 = loc("query_stride_0"(#loc))
#loc154 = loc("query_stride_1"(#loc))
#loc155 = loc("output_stride_0"(#loc))
#loc156 = loc("output_stride_1"(#loc))
#loc157 = loc("qq_bias_stride_0"(#loc))
#loc158 = loc("stride_k_cache_0"(#loc))
#loc159 = loc("stride_k_cache_1"(#loc))
#loc160 = loc("stride_k_cache_2"(#loc))
#loc161 = loc("stride_v_cache_0"(#loc))
#loc162 = loc("stride_v_cache_1"(#loc))
#loc163 = loc("stride_v_cache_2"(#loc))
#loc164 = loc("query_start_len_ptr"(#loc))
#loc165 = loc("num_seqs"(#loc))
#loc166 = loc("seq_idx"(#loc2))
#loc170 = loc("left"(#loc20))
#loc171 = loc("right"(#loc21))
#loc254 = loc("m_j"(#loc114))
#loc261 = loc("l_j"(#loc123))
#loc276 = loc(callsite(#loc170 at #loc166))
#loc277 = loc(callsite(#loc171 at #loc166))
#loc293 = loc(callsite(#loc1 at #loc254))
#loc295 = loc(callsite(#loc1 at #loc261))
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @kernel_unified_attention_2d(%output_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("output_ptr"(#loc)), %query_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("query_ptr"(#loc)), %key_cache_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("key_cache_ptr"(#loc)), %value_cache_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("value_cache_ptr"(#loc)), %sink_ptr: !tt.ptr<bf16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("sink_ptr"(#loc)), %block_tables_ptr: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("block_tables_ptr"(#loc)), %seq_lens_ptr: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("seq_lens_ptr"(#loc)), %scale: f32 loc("scale"(#loc)), %k_scale: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("k_scale"(#loc)), %v_scale: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("v_scale"(#loc)), %out_scale: f32 loc("out_scale"(#loc)), %softcap: i32 {tt.divisibility = 16 : i32} loc("softcap"(#loc)), %block_table_stride: i64 {tt.divisibility = 16 : i32} loc("block_table_stride"(#loc)), %query_stride_0: i64 {tt.divisibility = 16 : i32} loc("query_stride_0"(#loc)), %query_stride_1: i64 {tt.divisibility = 16 : i32} loc("query_stride_1"(#loc)), %output_stride_0: i64 {tt.divisibility = 16 : i32} loc("output_stride_0"(#loc)), %output_stride_1: i64 {tt.divisibility = 16 : i32} loc("output_stride_1"(#loc)), %qq_bias_stride_0: i64 {tt.divisibility = 16 : i32} loc("qq_bias_stride_0"(#loc)), %stride_k_cache_0: i64 {tt.divisibility = 16 : i32} loc("stride_k_cache_0"(#loc)), %stride_k_cache_1: i64 {tt.divisibility = 16 : i32} loc("stride_k_cache_1"(#loc)), %stride_k_cache_2: i64 {tt.divisibility = 16 : i32} loc("stride_k_cache_2"(#loc)), %stride_v_cache_0: i64 {tt.divisibility = 16 : i32} loc("stride_v_cache_0"(#loc)), %stride_v_cache_1: i64 {tt.divisibility = 16 : i32} loc("stride_v_cache_1"(#loc)), %stride_v_cache_2: i64 {tt.divisibility = 16 : i32} loc("stride_v_cache_2"(#loc)), %query_start_len_ptr: !tt.ptr<i32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("query_start_len_ptr"(#loc)), %num_seqs: i32 loc("num_seqs"(#loc))) attributes {noinline = false} {
    %true = arith.constant true loc(#loc1)
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %cst = arith.constant dense<8> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_0 = arith.constant dense<64> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_1 = arith.constant dense<1> : tensor<128x1xi32, #mma> loc(#loc1)
    %cst_2 = arith.constant dense<0xFF80> : tensor<128xbf16, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_3 = arith.constant dense<64> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
    %cst_4 = arith.constant dense<0> : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc1)
    %cst_5 = arith.constant dense<0> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc1)
    %cst_6 = arith.constant dense<1> : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc1)
    %cst_7 = arith.constant dense<1> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc1)
    %cst_8 = arith.constant dense<64> : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc1)
    %cst_9 = arith.constant dense<64> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc1)
    %cst_10 = arith.constant dense<1.000000e+00> : tensor<128x1xf32, #mma> loc(#loc1)
    %cst_11 = arith.constant dense<0.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_12 = arith.constant dense<0xFF800000> : tensor<128x64xf32, #mma> loc(#loc1)
    %cst_13 = arith.constant dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_14 = arith.constant dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_15 = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #mma> loc(#loc1)
    %seq_idx = arith.constant 2 : i32 loc(#loc273)
    %c63_i32 = arith.constant 63 : i32 loc(#loc1)
    %c64_i32 = arith.constant 64 : i32 loc(#loc1)
    %cst_16 = arith.constant dense<0.000000e+00> : tensor<128x64xbf16, #blocked> loc(#loc1)
    %c16_i32 = arith.constant 16 : i32 loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %c8_i32 = arith.constant 8 : i32 loc(#loc1)
    %cst_17 = arith.constant dense<8> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
    %cst_18 = arith.constant dense<8> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc1)
    %cst_19 = arith.constant dense<64> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc1)
    %cst_20 = arith.constant dense<1> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc1)
    %cst_21 = arith.constant dense<0> : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc1)
    %cst_22 = arith.constant dense<64> : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc1)
    %kv_head_idx = tt.get_program_id x : i32 loc(#loc167)
    %q_block_global_idx = tt.get_program_id y : i32 loc(#loc168)
    llvm.intr.assume %true : i1 loc(#loc5)
    llvm.intr.assume %true : i1 loc(#loc6)
    llvm.intr.assume %true : i1 loc(#loc7)
    llvm.intr.assume %true : i1 loc(#loc8)
    llvm.intr.assume %true : i1 loc(#loc9)
    llvm.intr.assume %true : i1 loc(#loc10)
    llvm.intr.assume %true : i1 loc(#loc11)
    llvm.intr.assume %true : i1 loc(#loc12)
    llvm.intr.assume %true : i1 loc(#loc13)
    llvm.intr.assume %true : i1 loc(#loc14)
    llvm.intr.assume %true : i1 loc(#loc15)
    llvm.intr.assume %true : i1 loc(#loc16)
    llvm.intr.assume %true : i1 loc(#loc17)
    %right:2 = scf.while (%left = %c0_i32, %right_136 = %num_seqs) : (i32, i32) -> (i32, i32) {
      %seq_idx_137 = arith.cmpi slt, %left, %right_136 : i32 loc(#loc275)
      scf.condition(%seq_idx_137) %left, %right_136 : i32, i32 loc(#loc275)
    } do {
    ^bb0(%left: i32 loc(callsite(#loc170 at #loc166)), %right_136: i32 loc(callsite(#loc171 at #loc166))):
      %mid = arith.addi %left, %right_136 : i32 loc(#loc278)
      %right_137 = arith.divsi %mid, %seq_idx : i32 loc(#loc297)
      %val = tt.addptr %query_start_len_ptr, %right_137 : !tt.ptr<i32>, i32 loc(#loc280)
      %val_138 = tt.load %val : !tt.ptr<i32> loc(#loc281)
      %mid_val = arith.divsi %val_138, %c16_i32 : i32 loc(#loc282)
      %mid_val_139 = arith.addi %mid_val, %right_137 : i32 loc(#loc283)
      %seq_idx_140 = arith.cmpi sle, %mid_val_139, %q_block_global_idx : i32 loc(#loc284)
      %seq_idx_141 = arith.select %seq_idx_140, %right_136, %right_137 : i32 loc(#loc285)
      %seq_idx_142 = scf.if %seq_idx_140 -> (i32) {
        %left_143 = arith.addi %right_137, %c1_i32 : i32 loc(#loc298)
        scf.yield %left_143 : i32 loc(#loc298)
      } else {
        scf.yield %left : i32 loc(#loc273)
      } loc(#loc285)
      scf.yield %seq_idx_142, %seq_idx_141 : i32, i32 loc(#loc287)
    } loc(#loc296)
    %seq_idx_23 = arith.subi %right#0, %c1_i32 : i32 loc(#loc288)
    %q_block_start_idx = tt.addptr %query_start_len_ptr, %seq_idx_23 : !tt.ptr<i32>, i32 loc(#loc179)
    %q_block_start_idx_24 = tt.load %q_block_start_idx : !tt.ptr<i32> loc(#loc180)
    %q_block_start_idx_25 = arith.divsi %q_block_start_idx_24, %c16_i32 : i32 loc(#loc181)
    %q_block_start_idx_26 = arith.addi %q_block_start_idx_25, %seq_idx_23 : i32 loc(#loc182)
    %q_block_local_idx = arith.subi %q_block_global_idx, %q_block_start_idx_26 : i32 loc(#loc183)
    %cur_batch_in_all_stop_index = tt.addptr %q_block_start_idx, %c1_i32 : !tt.ptr<i32>, i32 loc(#loc184)
    %cur_batch_in_all_stop_index_27 = tt.load %cur_batch_in_all_stop_index : !tt.ptr<i32> loc(#loc185)
    %cur_batch_query_len = arith.subi %cur_batch_in_all_stop_index_27, %q_block_start_idx_24 : i32 loc(#loc186)
    %0 = arith.muli %q_block_local_idx, %c16_i32 : i32 loc(#loc41)
    %1 = arith.cmpi sge, %0, %cur_batch_query_len : i32 loc(#loc42)
    cf.cond_br %1, ^bb1, ^bb2 loc(#loc42)
  ^bb1:  // pred: ^bb0
    tt.return loc(#loc43)
  ^bb2:  // pred: ^bb0
    %offs_m = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc187)
    %offs_m_28 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc187)
    %offs_m_29 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc187)
    %offs_d = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc188)
    %offs_d_30 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc188)
    %offs_d_31 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc188)
    %offs_d_32 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc188)
    %query_pos = arith.divsi %offs_m, %cst_18 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc189)
    %query_pos_33 = arith.divsi %offs_m_28, %cst_17 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc189)
    %query_pos_34 = arith.divsi %offs_m_29, %cst : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc189)
    %query_pos_35 = tt.splat %0 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc190)
    %query_pos_36 = tt.splat %0 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc190)
    %query_pos_37 = tt.splat %0 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc190)
    %query_pos_38 = arith.addi %query_pos_35, %query_pos : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc190)
    %query_pos_39 = arith.addi %query_pos_36, %query_pos_33 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc190)
    %query_pos_40 = arith.addi %query_pos_37, %query_pos_34 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc190)
    %query_offset_0 = tt.splat %q_block_start_idx_24 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc191)
    %query_offset_0_41 = tt.splat %q_block_start_idx_24 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc191)
    %query_offset_0_42 = arith.addi %query_offset_0, %query_pos_38 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc191)
    %query_offset_0_43 = arith.addi %query_offset_0_41, %query_pos_39 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc191)
    %query_offset_1 = arith.muli %kv_head_idx, %c8_i32 : i32 loc(#loc192)
    %query_offset_1_44 = arith.remsi %offs_m, %cst_18 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc193)
    %query_offset_1_45 = arith.remsi %offs_m_28, %cst_17 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc193)
    %query_offset_1_46 = arith.remsi %offs_m_29, %cst : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc193)
    %query_offset_1_47 = tt.splat %query_offset_1 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc194)
    %query_offset_1_48 = tt.splat %query_offset_1 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc194)
    %query_offset_1_49 = tt.splat %query_offset_1 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc194)
    %query_offset_1_50 = arith.addi %query_offset_1_47, %query_offset_1_44 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc194)
    %query_offset_1_51 = arith.addi %query_offset_1_48, %query_offset_1_45 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc194)
    %query_offset_1_52 = arith.addi %query_offset_1_49, %query_offset_1_46 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc194)
    %query_offset = tt.expand_dims %query_offset_0_42 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<128x1xi32, #linear> loc(#loc195)
    %query_offset_53 = tt.expand_dims %query_offset_0_43 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xi32, #blocked> loc(#loc195)
    %query_offset_54 = arith.extsi %query_offset : tensor<128x1xi32, #linear> to tensor<128x1xi64, #linear> loc(#loc196)
    %query_offset_55 = arith.extsi %query_offset_53 : tensor<128x1xi32, #blocked> to tensor<128x1xi64, #blocked> loc(#loc196)
    %query_offset_56 = tt.expand_dims %query_offset_1_50 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<128x1xi32, #linear> loc(#loc197)
    %query_offset_57 = tt.expand_dims %query_offset_1_51 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xi32, #blocked> loc(#loc197)
    %query_offset_58 = arith.extsi %query_offset_56 : tensor<128x1xi32, #linear> to tensor<128x1xi64, #linear> loc(#loc198)
    %query_offset_59 = arith.extsi %query_offset_57 : tensor<128x1xi32, #blocked> to tensor<128x1xi64, #blocked> loc(#loc198)
    %query_offset_60 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc199)
    %query_offset_61 = tt.expand_dims %offs_d {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> -> tensor<1x64xi32, #linear> loc(#loc199)
    %query_offset_62 = tt.expand_dims %offs_d_30 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi32, #blocked> loc(#loc199)
    %query_offset_63 = tt.expand_dims %query_offset_60 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x64xi32, #blocked1> loc(#loc199)
    %query_offset_64 = arith.extsi %query_offset_61 : tensor<1x64xi32, #linear> to tensor<1x64xi64, #linear> loc(#loc200)
    %query_offset_65 = arith.extsi %query_offset_62 : tensor<1x64xi32, #blocked> to tensor<1x64xi64, #blocked> loc(#loc200)
    %query_offset_66 = arith.extsi %query_offset_63 : tensor<1x64xi32, #blocked1> to tensor<1x64xi64, #blocked1> loc(#loc200)
    %dim_mask = arith.cmpi slt, %offs_d, %cst_19 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc201)
    %dim_mask_67 = arith.cmpi slt, %offs_d_30, %cst_9 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc201)
    %dim_mask_68 = arith.cmpi slt, %offs_d_32, %cst_8 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc201)
    %dim_mask_69 = arith.select %dim_mask, %cst_20, %cst_21 : tensor<64xi1, #ttg.slice<{dim = 0, parent = #linear}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc202)
    %dim_mask_70 = arith.select %dim_mask_67, %cst_7, %cst_5 : tensor<64xi1, #ttg.slice<{dim = 0, parent = #blocked}>>, tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc202)
    %dim_mask_71 = arith.select %dim_mask_68, %cst_6, %cst_4 : tensor<64xi1, #ttg.slice<{dim = 1, parent = #blocked1}>>, tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc202)
    %dim_mask_72 = arith.cmpi ne, %dim_mask_69, %cst_21 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc203)
    %dim_mask_73 = arith.cmpi ne, %dim_mask_70, %cst_5 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc203)
    %dim_mask_74 = arith.cmpi ne, %dim_mask_71, %cst_4 : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc203)
    %query_mask_0 = tt.splat %cur_batch_query_len : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc204)
    %query_mask_0_75 = tt.splat %cur_batch_query_len : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc204)
    %query_mask_0_76 = tt.splat %cur_batch_query_len : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc204)
    %query_mask_0_77 = arith.cmpi slt, %query_pos_38, %query_mask_0 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc204)
    %query_mask_0_78 = arith.cmpi slt, %query_pos_39, %query_mask_0_75 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc204)
    %query_mask_0_79 = arith.cmpi slt, %query_pos_40, %query_mask_0_76 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc204)
    %query_mask_1 = arith.cmpi slt, %query_offset_1_50, %cst_22 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc205)
    %query_mask_1_80 = arith.cmpi slt, %query_offset_1_51, %cst_3 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc205)
    %query_mask_1_81 = arith.cmpi slt, %query_offset_1_52, %cst_0 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc205)
    %Q = tt.expand_dims %dim_mask_72 {axis = 0 : i32} : tensor<64xi1, #ttg.slice<{dim = 0, parent = #linear}>> -> tensor<1x64xi1, #linear> loc(#loc206)
    %Q_82 = tt.expand_dims %dim_mask_73 {axis = 0 : i32} : tensor<64xi1, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi1, #blocked> loc(#loc206)
    %Q_83 = tt.expand_dims %query_mask_0_77 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<128x1xi1, #linear> loc(#loc207)
    %Q_84 = tt.expand_dims %query_mask_0_78 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xi1, #blocked> loc(#loc207)
    %Q_85 = tt.expand_dims %query_mask_0_79 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xi1, #mma> loc(#loc207)
    %Q_86 = tt.broadcast %Q : tensor<1x64xi1, #linear> -> tensor<128x64xi1, #linear> loc(#loc208)
    %Q_87 = tt.broadcast %Q_82 : tensor<1x64xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc208)
    %Q_88 = tt.broadcast %Q_83 : tensor<128x1xi1, #linear> -> tensor<128x64xi1, #linear> loc(#loc208)
    %Q_89 = tt.broadcast %Q_84 : tensor<128x1xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc208)
    %Q_90 = arith.andi %Q_86, %Q_88 : tensor<128x64xi1, #linear> loc(#loc208)
    %Q_91 = arith.andi %Q_87, %Q_89 : tensor<128x64xi1, #blocked> loc(#loc208)
    %Q_92 = tt.expand_dims %query_mask_1 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<128x1xi1, #linear> loc(#loc209)
    %Q_93 = tt.expand_dims %query_mask_1_80 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xi1, #blocked> loc(#loc209)
    %Q_94 = tt.expand_dims %query_mask_1_81 {axis = 1 : i32} : tensor<128xi1, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xi1, #mma> loc(#loc209)
    %Q_95 = tt.broadcast %Q_92 : tensor<128x1xi1, #linear> -> tensor<128x64xi1, #linear> loc(#loc210)
    %Q_96 = tt.broadcast %Q_93 : tensor<128x1xi1, #blocked> -> tensor<128x64xi1, #blocked> loc(#loc210)
    %Q_97 = arith.andi %Q_90, %Q_95 : tensor<128x64xi1, #linear> loc(#loc210)
    %Q_98 = arith.andi %Q_91, %Q_96 : tensor<128x64xi1, #blocked> loc(#loc210)
    %Q_99 = tt.splat %query_stride_0 : i64 -> tensor<128x1xi64, #blocked> loc(#loc211)
    %Q_100 = arith.muli %query_offset_55, %Q_99 : tensor<128x1xi64, #blocked> loc(#loc211)
    %Q_101 = tt.splat %query_stride_1 : i64 -> tensor<128x1xi64, #blocked> loc(#loc211)
    %Q_102 = arith.muli %query_offset_59, %Q_101 : tensor<128x1xi64, #blocked> loc(#loc211)
    %Q_103 = arith.addi %Q_100, %Q_102 : tensor<128x1xi64, #blocked> loc(#loc211)
    %Q_104 = tt.broadcast %Q_103 : tensor<128x1xi64, #blocked> -> tensor<128x64xi64, #blocked> loc(#loc211)
    %Q_105 = tt.broadcast %query_offset_65 : tensor<1x64xi64, #blocked> -> tensor<128x64xi64, #blocked> loc(#loc211)
    %Q_106 = arith.addi %Q_104, %Q_105 : tensor<128x64xi64, #blocked> loc(#loc211)
    %Q_107 = arith.trunci %Q_106 : tensor<128x64xi64, #blocked> to tensor<128x64xi32, #blocked> loc(#loc211)
    %Q_108 = tt.splat %query_ptr : !tt.ptr<bf16> -> tensor<128x64x!tt.ptr<bf16>, #blocked> loc(#loc212)
    %Q_109 = tt.addptr %Q_108, %Q_107 : tensor<128x64x!tt.ptr<bf16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc212)
    %Q_110 = tt.load %Q_109, %Q_98, %cst_16 cacheModifier = cg : tensor<128x64x!tt.ptr<bf16>, #blocked> loc(#loc212)
    %Q_111 = ttg.local_alloc %Q_110 : (tensor<128x64xbf16, #blocked>) -> !ttg.memdesc<128x64xbf16, #shared, #smem> loc(#loc212)
    %Q_112 = ttg.local_load %Q_111 : !ttg.memdesc<128x64xbf16, #shared, #smem> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc212)
    %block_table_offset = arith.extsi %seq_idx_23 : i32 to i64 loc(#loc213)
    %block_table_offset_113 = arith.muli %block_table_offset, %block_table_stride : i64 loc(#loc213)
    %M = tt.addptr %sink_ptr, %query_offset_1 : !tt.ptr<bf16>, i32 loc(#loc214)
    %M_114 = amdgpu.buffer_load %M[%query_offset_1_46], %query_mask_1_81, %cst_2 : tensor<128xbf16, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc215)
    %M_115 = arith.extf %M_114 : tensor<128xbf16, #ttg.slice<{dim = 1, parent = #mma}>> to tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc216)
    %seq_len = tt.addptr %seq_lens_ptr, %seq_idx_23 : !tt.ptr<i32>, i32 loc(#loc217)
    %seq_len_116 = tt.load %seq_len : !tt.ptr<i32> loc(#loc218)
    %context_len = arith.subi %seq_len_116, %cur_batch_query_len : i32 loc(#loc219)
    %max_seq_prefix_len = arith.addi %context_len, %0 : i32 loc(#loc220)
    %max_seq_prefix_len_117 = arith.addi %max_seq_prefix_len, %c16_i32 : i32 loc(#loc221)
    %max_seq_prefix_len_118 = arith.minsi %max_seq_prefix_len_117, %seq_len_116 : i32 loc(#loc222)
    %num_blocks = arith.addi %max_seq_prefix_len_118, %c63_i32 : i32 loc(#loc289)
    %num_blocks_119 = arith.divsi %num_blocks, %c64_i32 : i32 loc(#loc290)
    %physical_block_idx = tt.addptr %block_tables_ptr, %block_table_offset_113 : !tt.ptr<i32>, i64 loc(#loc224)
    %v_offset = arith.extsi %kv_head_idx : i32 to i64 loc(#loc225)
    %v_offset_120 = arith.muli %v_offset, %stride_v_cache_2 : i64 loc(#loc225)
    %v_offset_121 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc226)
    %v_offset_122 = tt.expand_dims %offs_d_32 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<64x1xi32, #blocked1> loc(#loc226)
    %v_offset_123 = tt.expand_dims %v_offset_121 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xi32, #blocked> loc(#loc226)
    %v_offset_124 = arith.extsi %v_offset_122 : tensor<64x1xi32, #blocked1> to tensor<64x1xi64, #blocked1> loc(#loc227)
    %v_offset_125 = arith.extsi %v_offset_123 : tensor<64x1xi32, #blocked> to tensor<64x1xi64, #blocked> loc(#loc227)
    %k_offset = arith.muli %v_offset, %stride_k_cache_2 : i64 loc(#loc228)
    %K_load = tt.expand_dims %dim_mask_74 {axis = 1 : i32} : tensor<64xi1, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<64x1xi1, #blocked1> loc(#loc229)
    %K_load_126 = tt.broadcast %K_load : tensor<64x1xi1, #blocked1> -> tensor<64x64xi1, #blocked1> loc(#loc230)
    %V_load = tt.broadcast %Q_82 : tensor<1x64xi1, #blocked> -> tensor<64x64xi1, #blocked> loc(#loc231)
    %seq_mask = tt.expand_dims %query_pos_40 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xi32, #mma> loc(#loc232)
    %seq_mask_127 = tt.splat %context_len : i32 -> tensor<128x1xi32, #mma> loc(#loc233)
    %seq_mask_128 = arith.addi %seq_mask_127, %seq_mask : tensor<128x1xi32, #mma> loc(#loc233)
    %seq_mask_129 = arith.addi %seq_mask_128, %cst_1 : tensor<128x1xi32, #mma> loc(#loc234)
    %seq_mask_130 = tt.broadcast %seq_mask_129 : tensor<128x1xi32, #mma> -> tensor<128x64xi32, #mma> loc(#loc235)
    %S = tt.splat %scale : f32 -> tensor<128x64xf32, #mma> loc(#loc236)
    %S_131 = arith.andi %Q_94, %Q_85 : tensor<128x1xi1, #mma> loc(#loc237)
    %S_132 = tt.broadcast %S_131 : tensor<128x1xi1, #mma> -> tensor<128x64xi1, #mma> loc(#loc238)
    %acc:3 = scf.for %acc_136 = %c0_i32 to %num_blocks_119 step %c1_i32 iter_args(%M_137 = %M_115, %arg28 = %cst_13, %arg29 = %cst_15) -> (tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128x64xf32, #mma>)  : i32 {
      %physical_block_idx_138 = tt.addptr %physical_block_idx, %acc_136 : !tt.ptr<i32>, i32 loc(#loc240)
      %physical_block_idx_139 = tt.load %physical_block_idx_138 : !tt.ptr<i32> loc(#loc241)
      %v_offset_140 = arith.extsi %physical_block_idx_139 : i32 to i64 loc(#loc242)
      %v_offset_141 = arith.muli %v_offset_140, %stride_v_cache_0 : i64 loc(#loc242)
      %v_offset_142 = arith.addi %v_offset_141, %v_offset_120 : i64 loc(#loc243)
      %k_offset_143 = arith.muli %v_offset_140, %stride_k_cache_0 : i64 loc(#loc244)
      %k_offset_144 = arith.addi %k_offset_143, %k_offset : i64 loc(#loc245)
      %K_load_145 = tt.broadcast %v_offset_124 : tensor<64x1xi64, #blocked1> -> tensor<64x64xi64, #blocked1> loc(#loc246)
      %K_load_146 = tt.splat %stride_k_cache_1 : i64 -> tensor<1x64xi64, #blocked1> loc(#loc246)
      %K_load_147 = arith.muli %query_offset_66, %K_load_146 : tensor<1x64xi64, #blocked1> loc(#loc246)
      %K_load_148 = tt.broadcast %K_load_147 : tensor<1x64xi64, #blocked1> -> tensor<64x64xi64, #blocked1> loc(#loc246)
      %K_load_149 = arith.addi %K_load_145, %K_load_148 : tensor<64x64xi64, #blocked1> loc(#loc246)
      %K_load_150 = tt.addptr %key_cache_ptr, %k_offset_144 : !tt.ptr<bf16>, i64 loc(#loc246)
      %K_load_151 = arith.trunci %K_load_149 : tensor<64x64xi64, #blocked1> to tensor<64x64xi32, #blocked1> loc(#loc246)
      %K_load_152 = amdgpu.buffer_load %K_load_150[%K_load_151], %K_load_126 : tensor<64x64xbf16, #blocked1> loc(#loc230)
      %V_load_153 = tt.broadcast %query_offset_65 : tensor<1x64xi64, #blocked> -> tensor<64x64xi64, #blocked> loc(#loc247)
      %V_load_154 = tt.splat %stride_v_cache_1 : i64 -> tensor<64x1xi64, #blocked> loc(#loc247)
      %V_load_155 = arith.muli %v_offset_125, %V_load_154 : tensor<64x1xi64, #blocked> loc(#loc247)
      %V_load_156 = tt.broadcast %V_load_155 : tensor<64x1xi64, #blocked> -> tensor<64x64xi64, #blocked> loc(#loc247)
      %V_load_157 = arith.addi %V_load_153, %V_load_156 : tensor<64x64xi64, #blocked> loc(#loc247)
      %V_load_158 = tt.addptr %value_cache_ptr, %v_offset_142 : !tt.ptr<bf16>, i64 loc(#loc247)
      %V_load_159 = arith.trunci %V_load_157 : tensor<64x64xi64, #blocked> to tensor<64x64xi32, #blocked> loc(#loc247)
      %V_load_160 = amdgpu.buffer_load %V_load_158[%V_load_159], %V_load : tensor<64x64xbf16, #blocked> loc(#loc231)
      %seq_offset = arith.muli %acc_136, %c64_i32 : i32 loc(#loc248)
      %seq_offset_161 = tt.splat %seq_offset : i32 -> tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc249)
      %seq_offset_162 = arith.addi %seq_offset_161, %offs_d_31 : tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> loc(#loc249)
      %seq_mask_163 = tt.expand_dims %seq_offset_162 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #mma}>> -> tensor<1x64xi32, #mma> loc(#loc250)
      %seq_mask_164 = tt.broadcast %seq_mask_163 : tensor<1x64xi32, #mma> -> tensor<128x64xi32, #mma> loc(#loc235)
      %seq_mask_165 = arith.cmpi slt, %seq_mask_164, %seq_mask_130 : tensor<128x64xi32, #mma> loc(#loc235)
      %K_load_166 = ttg.local_alloc %K_load_152 : (tensor<64x64xbf16, #blocked1>) -> !ttg.memdesc<64x64xbf16, #shared1, #smem> loc(#loc230)
      %K_load_167 = ttg.local_load %K_load_166 : !ttg.memdesc<64x64xbf16, #shared1, #smem> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc230)
      %S_168 = tt.dot %Q_112, %K_load_167, %cst_15 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<128x64xf32, #mma> loc(#loc251)
      %S_169 = arith.mulf %S, %S_168 : tensor<128x64xf32, #mma> loc(#loc236)
      %S_170 = arith.addf %S_169, %cst_15 : tensor<128x64xf32, #mma> loc(#loc252)
      %S_171 = arith.andi %S_132, %seq_mask_165 : tensor<128x64xi1, #mma> loc(#loc238)
      %S_172 = arith.select %S_171, %S_170, %cst_12 : tensor<128x64xi1, #mma>, tensor<128x64xf32, #mma> loc(#loc253)
      %m_j = "tt.reduce"(%S_172) <{axis = 1 : i32}> ({
      ^bb0(%m_j_189: f32 loc(callsite(#loc1 at #loc254)), %m_j_190: f32 loc(callsite(#loc1 at #loc254))):
        %m_j_191 = arith.maxnumf %m_j_189, %m_j_190 : f32 loc(#loc300)
        tt.reduce.return %m_j_191 : f32 loc(#loc292)
      }) : (tensor<128x64xf32, #mma>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc292)
      %m_j_173 = arith.maxnumf %M_137, %m_j : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc255)
      %m_j_174 = arith.cmpf ogt, %m_j_173, %cst_14 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc256)
      %m_j_175 = arith.select %m_j_174, %m_j_173, %cst_11 : tensor<128xi1, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc257)
      %P = tt.expand_dims %m_j_175 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc258)
      %P_176 = tt.broadcast %P : tensor<128x1xf32, #mma> -> tensor<128x64xf32, #mma> loc(#loc259)
      %P_177 = arith.subf %S_172, %P_176 : tensor<128x64xf32, #mma> loc(#loc259)
      %P_178 = math.exp %P_177 : tensor<128x64xf32, #mma> loc(#loc260)
      %l_j = "tt.reduce"(%P_178) <{axis = 1 : i32}> ({
      ^bb0(%l_j_189: f32 loc(callsite(#loc1 at #loc261)), %l_j_190: f32 loc(callsite(#loc1 at #loc261))):
        %l_j_191 = arith.addf %l_j_189, %l_j_190 : f32 loc(#loc301)
        tt.reduce.return %l_j_191 : f32 loc(#loc294)
      }) : (tensor<128x64xf32, #mma>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc294)
      %alpha = arith.subf %M_137, %m_j_175 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc262)
      %alpha_179 = math.exp %alpha : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc263)
      %acc_180 = tt.expand_dims %alpha_179 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc264)
      %acc_181 = tt.broadcast %acc_180 : tensor<128x1xf32, #mma> -> tensor<128x64xf32, #mma> loc(#loc265)
      %acc_182 = arith.mulf %arg29, %acc_181 : tensor<128x64xf32, #mma> loc(#loc265)
      %L = arith.mulf %arg28, %alpha_179 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc266)
      %L_183 = arith.addf %L, %l_j : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> loc(#loc267)
      %acc_184 = arith.truncf %P_178 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc268)
      %acc_185 = ttg.convert_layout %acc_184 : tensor<128x64xbf16, #mma> -> tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> loc(#loc268)
      %V_load_186 = ttg.local_alloc %V_load_160 : (tensor<64x64xbf16, #blocked>) -> !ttg.memdesc<64x64xbf16, #shared2, #smem> loc(#loc231)
      %V_load_187 = ttg.local_load %V_load_186 : !ttg.memdesc<64x64xbf16, #shared2, #smem> -> tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> loc(#loc231)
      %acc_188 = tt.dot %acc_185, %V_load_187, %acc_182 : tensor<128x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<64x64xbf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<128x64xf32, #mma> loc(#loc269)
      scf.yield %m_j_175, %L_183, %acc_188 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>>, tensor<128x64xf32, #mma> loc(#loc133)
    } loc(#loc299)
    %one_over_L = tt.expand_dims %acc#1 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc270)
    %one_over_L_133 = arith.divf %cst_10, %one_over_L : tensor<128x1xf32, #mma> loc(#loc271)
    %acc_134 = tt.broadcast %one_over_L_133 : tensor<128x1xf32, #mma> -> tensor<128x64xf32, #mma> loc(#loc272)
    %acc_135 = arith.mulf %acc#2, %acc_134 : tensor<128x64xf32, #mma> loc(#loc272)
    %2 = tt.splat %output_stride_0 : i64 -> tensor<128x1xi64, #linear> loc(#loc137)
    %3 = arith.muli %query_offset_54, %2 : tensor<128x1xi64, #linear> loc(#loc137)
    %4 = tt.splat %output_stride_1 : i64 -> tensor<128x1xi64, #linear> loc(#loc137)
    %5 = arith.muli %query_offset_58, %4 : tensor<128x1xi64, #linear> loc(#loc137)
    %6 = arith.addi %3, %5 : tensor<128x1xi64, #linear> loc(#loc137)
    %7 = tt.broadcast %6 : tensor<128x1xi64, #linear> -> tensor<128x64xi64, #linear> loc(#loc137)
    %8 = tt.broadcast %query_offset_64 : tensor<1x64xi64, #linear> -> tensor<128x64xi64, #linear> loc(#loc137)
    %9 = arith.addi %7, %8 : tensor<128x64xi64, #linear> loc(#loc137)
    %10 = arith.trunci %9 : tensor<128x64xi64, #linear> to tensor<128x64xi32, #linear> loc(#loc137)
    %11 = arith.truncf %acc_135 : tensor<128x64xf32, #mma> to tensor<128x64xbf16, #mma> loc(#loc138)
    %12 = ttg.convert_layout %11 : tensor<128x64xbf16, #mma> -> tensor<128x64xbf16, #linear> loc(#loc138)
    %13 = tt.splat %output_ptr : !tt.ptr<bf16> -> tensor<128x64x!tt.ptr<bf16>, #linear> loc(#loc138)
    %14 = tt.addptr %13, %10 : tensor<128x64x!tt.ptr<bf16>, #linear>, tensor<128x64xi32, #linear> loc(#loc138)
    tt.store %14, %12, %Q_97 : tensor<128x64x!tt.ptr<bf16>, #linear> loc(#loc138)
    tt.return loc(#loc139)
  } loc(#loc)
} loc(#loc)
#loc3 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":98:32)
#loc4 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":99:39)
#loc5 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":101:14)
#loc6 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":102:14)
#loc7 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":103:14)
#loc8 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":104:14)
#loc9 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":105:14)
#loc10 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":106:14)
#loc11 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":107:14)
#loc12 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":108:14)
#loc13 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":109:14)
#loc14 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":110:14)
#loc15 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":111:14)
#loc16 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":112:14)
#loc17 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":113:14)
#loc18 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":36:4)
#loc19 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":36:17)
#loc22 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":37:22)
#loc23 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":37:32)
#loc24 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":38:44)
#loc25 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":38:22)
#loc26 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":39:25)
#loc27 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":39:35)
#loc28 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":41:22)
#loc29 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":41:11)
#loc30 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":42:25)
#loc31 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":41:8)
#loc32 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":46:18)
#loc33 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":119:54)
#loc34 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":119:32)
#loc35 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":119:66)
#loc36 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":119:76)
#loc37 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":121:45)
#loc38 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":124:74)
#loc39 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":124:42)
#loc40 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":126:56)
#loc41 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":128:27)
#loc42 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":128:38)
#loc43 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":129:8)
#loc44 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":131:26)
#loc45 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":132:26)
#loc46 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":133:56)
#loc47 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":133:46)
#loc48 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":135:52)
#loc49 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":136:35)
#loc50 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":136:65)
#loc51 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":136:56)
#loc52 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":138:23)
#loc53 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":138:34)
#loc54 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":139:25)
#loc55 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":139:36)
#loc56 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":140:17)
#loc57 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":140:10)
#loc58 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":143:33)
#loc59 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":143:47)
#loc60 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":143:53)
#loc61 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":144:31)
#loc62 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":145:36)
#loc63 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":155:22)
#loc64 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":155:46)
#loc65 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":155:33)
#loc66 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":155:70)
#loc67 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":155:57)
#loc68 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":154:20)
#loc69 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":154:8)
#loc70 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":160:35)
#loc71 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":166:23)
#loc72 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":166:12)
#loc73 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":169:13)
#loc74 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":175:37)
#loc75 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":175:22)
#loc76 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":178:28)
#loc77 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":196:10)
#loc78 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":198:10)
#loc79 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":203:56)
#loc80 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":15:20)
#loc81 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":208:45)
#loc82 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":15:26)
#loc83 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":220:56)
#loc84 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":226:28)
#loc85 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":228:21)
#loc86 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":228:32)
#loc87 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":233:28)
#loc88 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":241:26)
#loc89 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":240:12)
#loc90 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":256:12)
#loc91 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":272:65)
#loc92 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":272:55)
#loc93 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":272:76)
#loc94 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":272:41)
#loc95 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":277:21)
#loc96 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":283:36)
#loc97 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":283:60)
#loc98 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":218:37)
#loc99 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":220:77)
#loc100 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":220:37)
#loc101 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":225:33)
#loc102 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":226:14)
#loc103 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":232:33)
#loc104 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":233:14)
#loc105 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":240:28)
#loc106 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":256:30)
#loc107 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":270:25)
#loc108 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":270:38)
#loc109 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":272:30)
#loc110 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":277:31)
#loc111 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":277:13)
#loc112 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":283:73)
#loc113 = loc("/app/OAI-triton/python/triton/language/standard.py":189:40)
#loc115 = loc("/app/OAI-triton/python/triton/language/standard.py":168:27)
#loc116 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":310:28)
#loc117 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":313:29)
#loc118 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":313:49)
#loc119 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":316:27)
#loc120 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":316:23)
#loc121 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":316:19)
#loc122 = loc("/app/OAI-triton/python/triton/language/standard.py":291:36)
#loc124 = loc("/app/OAI-triton/python/triton/language/standard.py":261:15)
#loc125 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":322:27)
#loc126 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":322:23)
#loc127 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":325:26)
#loc128 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":325:20)
#loc129 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":328:16)
#loc130 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":328:24)
#loc131 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":332:27)
#loc132 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":332:37)
#loc133 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":332:8)
#loc134 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":335:25)
#loc135 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":335:23)
#loc136 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":336:16)
#loc137 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":348:21)
#loc138 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":349:8)
#loc139 = loc("/app/OAI-triton/unified_attn_ubench/unified_attention_aiter.py":347:4)
#loc167 = loc("kv_head_idx"(#loc3))
#loc168 = loc("q_block_global_idx"(#loc4))
#loc169 = loc("left"(#loc18))
#loc172 = loc("mid"(#loc22))
#loc173 = loc("mid"(#loc23))
#loc174 = loc("val"(#loc24))
#loc175 = loc("val"(#loc25))
#loc176 = loc("mid_val"(#loc26))
#loc177 = loc("mid_val"(#loc27))
#loc178 = loc("left"(#loc30))
#loc179 = loc("q_block_start_idx"(#loc33))
#loc180 = loc("q_block_start_idx"(#loc34))
#loc181 = loc("q_block_start_idx"(#loc35))
#loc182 = loc("q_block_start_idx"(#loc36))
#loc183 = loc("q_block_local_idx"(#loc37))
#loc184 = loc("cur_batch_in_all_stop_index"(#loc38))
#loc185 = loc("cur_batch_in_all_stop_index"(#loc39))
#loc186 = loc("cur_batch_query_len"(#loc40))
#loc187 = loc("offs_m"(#loc44))
#loc188 = loc("offs_d"(#loc45))
#loc189 = loc("query_pos"(#loc46))
#loc190 = loc("query_pos"(#loc47))
#loc191 = loc("query_offset_0"(#loc48))
#loc192 = loc("query_offset_1"(#loc49))
#loc193 = loc("query_offset_1"(#loc50))
#loc194 = loc("query_offset_1"(#loc51))
#loc195 = loc("query_offset"(#loc52))
#loc196 = loc("query_offset"(#loc53))
#loc197 = loc("query_offset"(#loc54))
#loc198 = loc("query_offset"(#loc55))
#loc199 = loc("query_offset"(#loc56))
#loc200 = loc("query_offset"(#loc57))
#loc201 = loc("dim_mask"(#loc58))
#loc202 = loc("dim_mask"(#loc59))
#loc203 = loc("dim_mask"(#loc60))
#loc204 = loc("query_mask_0"(#loc61))
#loc205 = loc("query_mask_1"(#loc62))
#loc206 = loc("Q"(#loc63))
#loc207 = loc("Q"(#loc64))
#loc208 = loc("Q"(#loc65))
#loc209 = loc("Q"(#loc66))
#loc210 = loc("Q"(#loc67))
#loc211 = loc("Q"(#loc68))
#loc212 = loc("Q"(#loc69))
#loc213 = loc("block_table_offset"(#loc70))
#loc214 = loc("M"(#loc71))
#loc215 = loc("M"(#loc72))
#loc216 = loc("M"(#loc73))
#loc217 = loc("seq_len"(#loc74))
#loc218 = loc("seq_len"(#loc75))
#loc219 = loc("context_len"(#loc76))
#loc220 = loc("max_seq_prefix_len"(#loc77))
#loc221 = loc("max_seq_prefix_len"(#loc78))
#loc222 = loc("max_seq_prefix_len"(#loc79))
#loc223 = loc("num_blocks"(#loc81))
#loc224 = loc("physical_block_idx"(#loc83))
#loc225 = loc("v_offset"(#loc84))
#loc226 = loc("v_offset"(#loc85))
#loc227 = loc("v_offset"(#loc86))
#loc228 = loc("k_offset"(#loc87))
#loc229 = loc("K_load"(#loc88))
#loc230 = loc("K_load"(#loc89))
#loc231 = loc("V_load"(#loc90))
#loc232 = loc("seq_mask"(#loc91))
#loc233 = loc("seq_mask"(#loc92))
#loc234 = loc("seq_mask"(#loc93))
#loc235 = loc("seq_mask"(#loc94))
#loc236 = loc("S"(#loc95))
#loc237 = loc("S"(#loc96))
#loc238 = loc("S"(#loc97))
#loc239 = loc("M"(#loc98))
#loc240 = loc("physical_block_idx"(#loc99))
#loc241 = loc("physical_block_idx"(#loc100))
#loc242 = loc("v_offset"(#loc101))
#loc243 = loc("v_offset"(#loc102))
#loc244 = loc("k_offset"(#loc103))
#loc245 = loc("k_offset"(#loc104))
#loc246 = loc("K_load"(#loc105))
#loc247 = loc("V_load"(#loc106))
#loc248 = loc("seq_offset"(#loc107))
#loc249 = loc("seq_offset"(#loc108))
#loc250 = loc("seq_mask"(#loc109))
#loc251 = loc("S"(#loc110))
#loc252 = loc("S"(#loc111))
#loc253 = loc("S"(#loc112))
#loc255 = loc("m_j"(#loc116))
#loc256 = loc("m_j"(#loc117))
#loc257 = loc("m_j"(#loc118))
#loc258 = loc("P"(#loc119))
#loc259 = loc("P"(#loc120))
#loc260 = loc("P"(#loc121))
#loc262 = loc("alpha"(#loc125))
#loc263 = loc("alpha"(#loc126))
#loc264 = loc("acc"(#loc127))
#loc265 = loc("acc"(#loc128))
#loc266 = loc("L"(#loc129))
#loc267 = loc("L"(#loc130))
#loc268 = loc("acc"(#loc131))
#loc269 = loc("acc"(#loc132))
#loc270 = loc("one_over_L"(#loc134))
#loc271 = loc("one_over_L"(#loc135))
#loc272 = loc("acc"(#loc136))
#loc273 = loc(callsite(#loc1 at #loc166))
#loc274 = loc("right"(#loc169))
#loc275 = loc(callsite(#loc19 at #loc166))
#loc278 = loc(callsite(#loc172 at #loc166))
#loc279 = loc("right"(#loc173))
#loc280 = loc(callsite(#loc174 at #loc166))
#loc281 = loc(callsite(#loc175 at #loc166))
#loc282 = loc(callsite(#loc176 at #loc166))
#loc283 = loc(callsite(#loc177 at #loc166))
#loc284 = loc(callsite(#loc28 at #loc166))
#loc285 = loc(callsite(#loc29 at #loc166))
#loc286 = loc("left"(#loc178))
#loc287 = loc(callsite(#loc31 at #loc166))
#loc288 = loc(callsite(#loc32 at #loc166))
#loc289 = loc(callsite(#loc80 at #loc223))
#loc290 = loc(callsite(#loc82 at #loc223))
#loc291 = loc("L"(#loc239))
#loc292 = loc(callsite(#loc113 at #loc254))
#loc294 = loc(callsite(#loc122 at #loc261))
#loc296 = loc(callsite(#loc274 at #loc166))
#loc297 = loc(callsite(#loc279 at #loc166))
#loc298 = loc(callsite(#loc286 at #loc166))
#loc299 = loc("acc"(#loc291))
#loc300 = loc(callsite(#loc115 at #loc292))
#loc301 = loc(callsite(#loc124 at #loc294))
